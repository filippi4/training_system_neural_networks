-- phpMyAdmin SQL Dump
-- version 5.1.1
-- https://www.phpmyadmin.net/
--
-- Хост: 127.0.0.1:3306
-- Время создания: Май 03 2022 г., 03:09
-- Версия сервера: 10.6.5-MariaDB
-- Версия PHP: 7.4.27

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- База данных: `example_app`
--

-- --------------------------------------------------------

--
-- Структура таблицы `failed_jobs`
--

CREATE TABLE `failed_jobs` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `uuid` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `connection` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `queue` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `payload` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `exception` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `failed_at` timestamp NOT NULL DEFAULT current_timestamp()
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- Структура таблицы `lessons`
--

CREATE TABLE `lessons` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `title` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `content` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Дамп данных таблицы `lessons`
--

INSERT INTO `lessons` (`id`, `title`, `content`, `created_at`, `updated_at`) VALUES
(1, 'Нейронные сети: 1. Основы', '<a id=\"intro\"></a>\r\n<h2>Введение</h2>\r\n\r\n<p>\r\nЭтим документом начинается серия материалов, посвящённых нейронным сетям.\r\nИногда к ним относятся по принципу: человеческая нейронная сеть  может решать \r\nлюбые задачи, поэтому и достаточно большая искусственная нейронная сеть на это способна. \r\nЧаще всего архитектура сети и параметры её обучения являются предметом многочисленных экспериментов.\r\nСеть при этом оказывается чёрным ящиком, происходящее в котором  загадочно даже для её учителя.\r\n</p>\r\n<p>\r\nМы постараемся совмещать эмпирические советы и математическое понимания природы нейронов как\r\nразделяющих гиперплоскостей и функций нечёткой логики. \r\nСначала будут рассмотрены различные\r\nмодельные примеры двумерных пространств признаков. Наша цель - выработать интуитивное понимание\r\nвыбора архитектуры сети.\r\nВ дальнейшем мы перейдём к многомерным задачам, распознанию графических образов, свёрточным и  рекуррентным сетям.\r\n{{Приведенные ниже примеры можно <a href=\"NeuroNet2D.html\">запустить</a>, потренировавшись в подборе \r\nпараметров обучения. }}\r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"objs\"></a>\r\n<h2>Объекты и их признаки</h2>\r\n\r\n<p>\r\nПусть есть однотипные <b class=\"green\">объекты</b> (бутылки с вином, посетители в больнице, позиции на шахматной доске):\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/objs.png\" style=\"width:600px;\">\r\n</center>\r\n</p>\r\n<p>\r\nКаждый объект характеризуется набором (вектором) <b class=\"green\">признаков</b> <b>x</b><b class=\"norm\">={x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>}</b>.\r\nПризнаки могут быть:\r\n<ul>\r\n<li> вещественными (вес, рост)\r\n<li> бинарными (женщина/мужчина)\r\n<li> нечисловыми (красный,синий,...)\r\n</ul>\r\nДалее будем считать признаки вещественными числами из диапазона <b class=\"norm\">[0...1]</b>.\r\nЭтого всегда можно достичь при помощи нормировки, например: <b class=\"norm\">x -> (x-x<sub>min</sub>)/(x<sub>max</sub>-x<sub>min</sub>)</b>.\r\nБинарные признаки, соответственно, принимают значение <b class=\"norm\">0</b> или <b class=\"norm\">1</b>.\r\nНечисловые признаки, увеличив размерность вектора <b>x</b>, можно сделать бинарными (красный/не красный, синий/не синий).\r\nКроме этого, пока будем считать, что объекты между собой причинно не связаны и их порядок не существенен. \r\n</p>\r\n<p>\r\nПусть объекты данного типа  разбиваются на классы (человек: {здоровый, больной}, \r\nвино: {итальянское, французское, грузинское}).\r\nС каждым объектом можно также связать некоторое число <b class=\"norm\">y</b>\r\n (степень преимущества белых в шахматной позиции;\r\nкачество вина по усреднённому мнению экспертов и т.д.). Часто решаются следующие две, тесно связанные задачи:\r\n<ul>\r\n<li><b class=\"green\">Классификация:</b> к какому  из <b class=\"norm\">K</b>  классов принадлежит объект.<br>\r\n<li><b class=\"green\">Регрессия:</b> какое  число <b class=\"norm\">y</b> соответствует объекту.\r\n</ul>\r\n</p>\r\n<p>\r\nПримеры:\r\n<ul>\r\n<li> 1) 3 признака: <b>x</b>={температура, уровень гемоглобина, количество холестерина}; \r\n2 класса: {<b class=\"norm\">0</b>: здоровый, <b class=\"norm\">1</b>: больной};\r\n</li>\r\n<li> 2)  <b class=\"norm\">w*h</b> признаков: <b>x</b>={яркости  пикселей картинки шириной <b class=\"norm\">w</b> и высотой <b class=\"norm\">h</b>}; \r\n10 классов: {<b class=\"norm\">0-9</b>: цифра  на картинке}.\r\n</li>\r\n<li> 3)  <b class=\"norm\">8*8*13</b> признаков: <b>x</b>={коды шахматных фигур в ячейках}; \r\nрегрессия: {преимущество белых на чёрными = <b class=\"norm\">[-1...1]</b>}.\r\n</li>\r\n</ul>\r\n</p>\r\n\r\n<p>\r\nДля успешного решения задач классификации или регрессии, признаки, характеризующие\r\nобъект, должны быть значимыми, а вектор признаков - полным \r\n(достаточным для классификации объектов или определение регрессионной величины\r\n<b class=\"norm\">y</b>\r\n).\r\n</p>\r\n<p style=\"font-size:14px;   line-height:15px; margin:15px 50px; border: dashed 1px black; padding: 5px;\">\r\nОднажды Джед и Нед захотели различать своих лошадей.\r\nДжед сделал на ухе лошади царапину. \r\nНо лошадь Неда поцарапала о колючку тоже самое ухо. \r\nТогда Нед прицепил голубой бант на хвост своей лошади, но лошадь Джеда его сожрала. \r\nФермеры долго размышляли и выбрали признак, который не так легко изменить. \r\nОни тщательно измерил высоту лошадей, и оказалось,\r\nчто черная кобыла Джеда на один сантиметр выше белого жеребца Неда.\r\n</p>\r\n\r\n\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"box\"></a>\r\n<h2>Обучение с учителем</h2>\r\n\r\n<p>\r\nПусть есть множество объектов, каждый из которых принадлежит одному из <b class=\"norm\">k</b> пронумерованных\r\n <b class=\"norm\">(0,1,2,...k-1)</b> классов.\r\nНа этом <i>обучающем множестве</i>, предоставленнным  \"учителем\" (обычно человеком), система обучается. \r\nЗатем, для неизвестных системе объектов (<i>тестовом множестве</i>), она проводит их классификацию, \r\nт.е. сообщает к какому классу принадлежит данный объект.\r\nВ такой постановке - это задача   распознавания образов после обучения с учителем. \r\n</p>\r\n<p>\r\nЧисло признаков <b class=\"norm\">n</b> называется <i>размерностью пространства признаков</i>.\r\nПусть признаки лежат в диапазоне <b class=\"norm\">[0...1]</b>.\r\nТогда любой объект представим точкой внутри единичного <b class=\"norm\">n</b>-мерного куба в пространстве признаков.\r\n</p>\r\n\r\n<p>\r\n<img style=\"float:right; margin: 5px; width:200px\" src=\"https://synset.com/ai/ru/nn/im/blackbox.png\">\r\nРаспознающую систему  представим в виде чёрного ящика.\r\nУ этого ящика есть <b class=\"norm\">n</b> входов, на которые подаются значения признаков\r\n<b>x</b><b class=\"norm\">={x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>}</b> \r\nи <b class=\"norm\">k</b> выходов \r\n<b>y</b><b class=\"norm\">={y<sub>1</sub>,...,y<sub>k</sub>}</b>\r\n(по числу классов).\r\nЗначение выходов также будем считать вещественным числами из диапазона <b class=\"norm\">[0...1]</b>.\r\nСистема считается правильно обученной, если при подаче на входы признаков, соответствующих <b class=\"norm\">i</b>-тому\r\nклассу, значение  <b class=\"norm\">i</b>-того выхода равно <b class=\"norm\">1</b>, а всех остальных <b class=\"norm\">0</b>.\r\nНа практике, такого результата добиться трудно и все выходы оказываются отличными от нуля.\r\nТогда считается что номер выхода с максимальным значением и есть номер класса, а близость \r\nэтого значения к единице говорит о \"степени уверенности\" системы.\r\n</p>\r\n<p>\r\nКогда есть только два класса, ящик может иметь один выход. Если он равен  <b class=\"norm\">0</b>, то это один класс,\r\nа если <b class=\"norm\">1</b> - то другой. \r\nПри нечётком распознавании  вводятся пороги уверенности.\r\nНапример, если значение выхода лежит в диапазоне <b class=\"norm\">y=[0 ... 0.3]</b> - это первый класс, если <b class=\"norm\">y=[0.7 ... 1]</b> - второй,\r\nа при <b class=\"norm\">y=(0.3 ... 0.7)</b> система \"отказывается принимать решение\".\r\n</p>\r\n<p>\r\nЯщик с одним выходом может также аппроксимировать функцию <b class=\"norm\">y=f(x<sub>1</sub>,...,x<sub>n</sub>)</b>,\r\nзначения <b class=\"norm\">y</b> которой непрерывны и обычно также нормируются на единицу, т.е.\r\n<b class=\"norm\">y=[0 ... 1]</b>. В этом случае решается задача регрессии. \r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"neuron\"></a>\r\n<h2>Нейрон</h2>\r\n\r\n<p>\r\nНейронная сеть - одно из возможных наполнений чёрного ящика.\r\nУзел сети - это нейрон,  имеющий <b class=\"norm\">n</b> входов \r\n<b>x</b><b class=\"norm\">={x<sub>1</sub>,x<sub>2</sub>,...,x<sub>n</sub>}</b> \r\nи один выход <b class=\"norm\">y</b>. С каждым входом связан вещественный параметр <i>синаптического веса</i>\r\n<b>&omega;</b><b class=\"norm\">={w<sub>1</sub>,w<sub>2</sub>,...,w<sub>n</sub>}</b>.\r\nКроме этого, нейрон имеет также \"<i>параметр смещения</i>\" <b class=\"norm\">w<sub>0</sub></b>.\r\nТаким образом, любой нейрон с <b class=\"norm\">n</b> входами полностью определяется\r\n<b class=\"norm\">n+1</b>  параметром.\r\n</p>\r\n<p>\r\nВыход нейрона вычисляется следующим образом.\r\nЗначение каждого входа <b class=\"norm\">x<sub>i</sub></b>\r\nумножают на соответствующий ему синаптический вес  <b class=\"norm\">w<sub>i</sub></b> и эти произведения складывают.\r\nК сумме добавляют параметр смещения <b class=\"norm\">w<sub>0</sub></b>.\r\nРезультат <b class=\"norm\">d</b>\r\nприводят к диапазону <b class=\"norm\">[0 ... 1]</b> при помощи нелинейной\r\n<i>сигмоидной функции</i> <b class=\"norm\">y=S(d)</b>:\r\n</p>\r\n<center>\r\n<b class=\"norm\">\r\nd =  w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + ... + w<sub>n</sub>x<sub>n</sub>,\r\n&emsp;&emsp; &emsp; &emsp;&emsp;\r\ny = S(d) = 1/(1+exp(-d)).\r\n</b>\r\n</center>\r\n<p>\r\nСигмоидная функция\r\nстремится к <b class=\"norm\">1</b> при больших положительных <b class=\"norm\">d</b>\r\nи к <b class=\"norm\">0</b> при больших отрицательных <b class=\"norm\">d</b>.\r\nКогда <b class=\"norm\">d=0</b>, она равна <b class=\"norm\">S(0)=0.5</b>.\r\nТаким образом, нейрон это нелинейная функция <b class=\"norm\">n</b> переменных порогового вида:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/sigmoid.png\" style=\"width:700px;\">\r\n</center>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"neuron2\"></a>\r\n<h2>Сеть нейронов</h2>\r\n\r\n<p>\r\nСеть является множеством соединённых между собой нейронов.\r\nВозможны различные способы соединения и, следовательно, различные архитектуры сети.\r\nПусть нейроны располагаются слоями и значения выходов нейронов <b class=\"norm\">i</b>-того\r\nслоя подаётся на входы всех нейронов следующего <b class=\"norm\">i+1</b> слоя.\r\nТакую сеть называют полносвязной сетью прямого распространения.\r\nВходы сети мы будем обозначать квадратиками и называть входными нейронами.\r\nВ отличии от \"обычных\" нейронов - это просто линейная функция <b class=\"norm\">y=x</b>.\r\nВыходы нейронов последнего слоя сети являются выходами чёрного ящика и обозначаются треугольниками.\r\n</p>\r\n<p>\r\nНиже на первом рисунке сеть состоит из трёх входов (нулевой слой) и двух выходных нейронов.\r\nТакую архитектуру будем кодировать следующим образом: <b class=\"norm\">[3,2]</b>, где цифры - это число нейронов в слое.\r\nПервая цифра - всегда количество входов, а последняя - количество выходов.\r\nНа следующем рисунке представлена сеть <b class=\"norm\">[2,3,1]</b>. Она содержит один <i>скрытый слой</i>\r\nс тремя нейронами. Он скрыт в том смысле, что находится внутри чёрного ящика (пунктир) между входным и выходным слоем\r\n(нейроны выходного слоя, впрочем, также частично скрыты и наружу \"торчат\" только их выходы).\r\nНа третьем рисунке представлена сеть <b class=\"norm\">[2,3,3,2]</b> с двумя скрытыми слоями.\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/nets01.png\" style=\"width:600px;\">\r\n</center>\r\n<p>\r\nЭти сети названы сетями прямого распространения потому, что данные (признаки объекта) подаются\r\nна вход и последовательно, без петель, передаются (распространяются) к выходам.\r\nК такому же типу сетей относятся т.н. <i>свёрточные сети</i>, в которых соединены между собой \r\nне все нейроны двух соседних слоёв (ниже первый рисунок).\r\nЧасто при этом веса у всех нейронов свёрточного слоя одинаковые. Подробнее о таких сетях будет \r\nговориться при распознавании изображений. На втором рисунке ниже представлен вариант сети в которой\r\nпонятие слоя отсутствует, однако это по-прежнему сеть прямого распространения.\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/nets02.png\" style=\"width:600px;\">\r\n</center>\r\n<p>\r\nПоследний рисунок  - это уже сеть не прямого распространения, а т.н.\r\n<i>рекуррентная сеть</i>. В ней сигналы с одного или нескольких выходных нейронов подаются обратно на вход.\r\nОбычно такая рекурсия, проводится в несколько циклов, пока на выходах сети не установятся стационарные значения.\r\nРекуррентные сети обладают памятью и последовательность подачи объектов для них важна.\r\nТакое поведение полезно, если объекты упорядочены во времени (например при предсказании  временных рядов).\r\n</p>\r\n\r\n<p>\r\n<img src=\"https://synset.com/ai/ru/nn/im/460px-Neuron-rus.png\" style=\"float:right;width:330px;\">\r\nОбучение любой сети состоит в подборе параметров \r\n<b class=\"norm\">\r\nw<sub>0</sub>,w<sub>1</sub>,...,w<sub>n</sub></b>\r\n</b>\r\nкаждого нейрона, таким образом, чтобы  для данного объекта \r\n(подаём на входы сети <b class=\"norm\">x<sub>1</sub>,...,x<sub>n</sub></b>),\r\nвыходы сети имели значения, соответствующие классу объекта.\r\n</p>\r\n<p>\r\nОтметим, что, хотя нейрон всегда имеет только один выход, он может \"подаваться\" на входы различных нейронов.\r\nАналогично, в живых нейронах аксон расщепляется на отдельные отростки, \r\nкаждый их которых воздействует \r\nна синапсы (\"точки соединения\") дендридов других нейронов. \r\nЕсли нейрон возбудился, то это возбуждение передаётся по аксону к дендридам  его соседей.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"plane\"></a>\r\n<h2>Нейрон как гиперплоскость</h2>\r\n\r\n<p>\r\nЧтобы чёрный ящик распознающей системы сделать  прозрачнее, \r\nрассмотрим геометрическую интерпретацию нейрона.\r\nВ <b class=\"norm\">n</b>-мерном пространстве каждая точка задаётся <b class=\"norm\">n</b> координатами\r\n(вещественными числами <b>x</b><b class=\"norm\"> = {x<sub>1</sub>,...,x<sub>n</sub>}</b>).\r\nПлоскость  (как и в обычном 3-мерном пространстве) \r\nзадаётся вектором нормали \r\n<b>&omega;</b><b class=\"norm\">={w<sub>1</sub>,...,w<sub>n</sub>}</b> (перпендикуляр к плоскости)\r\nи произвольной точкой \r\n<b>x</b><b class=\"norm\"><sub>0</sub>={x<sub>01</sub>,...,x<sub>0n</sub>}</b>,\r\nлежащей в этой плоскости. Когда <b class=\"norm\">n &gt; 3</b>  плоскость принято называть <i>гиперплоскостью</i>.\r\n</p>\r\n<p>\r\nРасстояние <b class=\"norm\">d</b>\r\nот гиперплоскости до некоторой точки\r\n<b>x</b><b class=\"norm\">={x<sub>1</sub>,...,x<sub>n</sub>}</b> \r\nвычисляется по формуле\r\n</p>\r\n<center style=\"\">\r\n<b class=\"norm\">\r\nd = w<sub>0</sub> + w<sub>1</sub> x<sub>1</sub> + ... + w<sub>n</sub> x<sub>n</sub>,\r\n</b>\r\n&emsp;&emsp;&emsp;\r\nгде\r\n&emsp;&emsp;&emsp;\r\n<b class=\"norm\">\r\nw<sub>0</sub> = -(w<sub>1</sub> x<sub>01</sub> + ... + w<sub>n</sub> x<sub>0n</sub>).\r\n</b>\r\n</center>\r\n<p>\r\nПри этом <b class=\"norm\">d &gt; 0</b>, если точка <b>x</b> лежит с той стороны плоскости,\r\nкуда указывает вектор <b>&omega;</b> и <b class=\"norm\">d &lt; 0</b>, если с противоположной.\r\nКогда <b class=\"norm\">d = 0</b> - точка <b>x</b> лежит в плоскости.\r\nЭто ключевое для дальнейшего изложения утверждение, которое стоит запомнить. \r\n</p>\r\n<p>\r\nИзменение параметра <b class=\"norm\">w<sub>0</sub></b> сдвигает плоскость параллельным образом в пространстве.\r\nЕсли <b class=\"norm\">w<sub>0</sub></b> уменьшается, то плоскость смещается в направлении вектора <b>&omega;</b>\r\n(расстояние меньше),\r\nа если <b class=\"norm\">w<sub>0</sub></b> увеличивается - плоскость смещается против вектора <b>&omega;</b>.\r\nЭто непосредственно следует из приведенной выше формулы.\r\n</p>\r\n<p>\r\n<div style=\"border: 0.5px solid green; border-radius: 5px; padding:5px;\">\r\n<img style=\"float:right; margin: 5px; width:300px\" src=\"https://synset.com/ai/ru/nn/im/vec_plane.png\">\r\n&#9668; Вывод этой формулы (который можно пропустить) проведём в векторных обозначениях.\r\nЗапишем вектор <b class=\"norm\"><b>x - x<sub>0</sub></b></b>, начинающийся в точке <b class=\"norm\"><b>x<sub>0</sub></b></b> \r\n(лежащей в плоскости) \r\nи направленный в точку <b>x</b> (см. рисунок справа; векторы складываются по правилу треугольника).\r\nПоложение точки <b class=\"norm\"><b>x</b><sub>0</sub></b> выбрано в основании вектора <b>&omega;</b>,\r\nпоэтому <b>&omega;</b> и <b class=\"norm\"><b>x - x<sub>0</sub></b></b> коллинеарны (лежат на одной прямой).\r\nЕсли вектор <b>&omega;</b> единичный (<b>&omega;</b><sup>2</sup>=1), то скалярное произведение векторов \r\n<b class=\"norm\"><b>x - x<sub>0</sub></b></b> и <b>&omega;</b> равно расстоянию  точки <b>x</b> до плоскости: \r\n</p>\r\n<center style=\"margin-bottom:10px\">\r\n<b class=\"norm\">\r\nd = <b>w&middot;(x-x<sub>0</sub>)</b> = -<b>w&middot;x<sub>0</sub></b> + <b>w&middot;x</b> =  w<sub>0</sub> + <b>w&middot;x</b>.\r\n</b>\r\n</center>\r\n<p>\r\nЕсли длина  <b class=\"norm\">w=|</b><b>&omega;</b>| вектора <b>&omega;</b> нормали к плоскости \r\nотлична от единицы, то <b class=\"norm\">d</b> в <b class=\"norm\">w</b> раз больше (<b class=\"norm\">w&gt;1</b>)\r\nили меньше (<b class=\"norm\">w&lt;1</b> ) евклидового расстояния в <b class=\"norm\">n</b>-мерном пространстве.  \r\nКогда векторы\r\n<b class=\"norm\"><b>x - x<sub>0</sub></b></b> и <b>&omega;</b>\r\nнаправлены в противоположные стороны: <b class=\"norm\">d &lt; 0</b>.\r\n&#9658;\r\n</p>\r\n</div>\r\n<p>\r\n<img style=\"float:right; margin: 5px; width:150px\" src=\"https://synset.com/ai/ru/nn/im/plane.png\">\r\nЕсли пространство имеет <b class=\"norm\">n</b> измерений, \r\nто гиперплоскость это <b class=\"norm\">(n-1)</b>-мерный объект.<br> Она делит всё пространство на две\r\nчасти. \r\nДля наглядности рассмотрим 2-мерное пространство. \r\nГиперплоскостью в нём будет прямая линия (одномерный объект).\r\nСправа на рисунке кружок изображает одну точку пространства,\r\nа крестик - другую. Они расположены по разные стороны от линии (гиперплоскости).\r\nЕсли длина вектора <b>&omega;</b> много больше единицы, то и расстояния \r\n<b class=\"norm\">d</b>\r\nот точек к плоскости по модулю будут существенно большими единицы.\r\n</p>\r\n<p>\r\nВернёмся к нейрону. Несложно видеть, что он вычисляет расстояние <b class=\"norm\">d</b>\r\nот  точки с координатами  <b>x</b><b class=\"norm\">=(x<sub>1</sub>,...,x<sub>n</sub>)</b> (вектор входов)\r\nдо гиперплоскости (<b class=\"norm\">w<sub>0</sub></b>, <b>&omega;</b>). Параметры нейрона  <b>&omega;</b><b class=\"norm\">=(w<sub>1</sub>,...,w<sub>n</sub>)</b> \r\nопределяют направление нормали гиперплоскости, а <b class=\"norm\">w<sub>0</sub></b> \r\nсвязан со смещение плоскости вдоль вектора <b>&omega;</b>. На выход нейрона подаётся нормированное на диапазон <b class=\"norm\">[0...1]</b>\r\nрасстояние <b class=\"norm\">S(d)</b>. При больших <b class=\"norm\">w<sub>i</sub></b> объект, \r\nнарисованный выше кружком,  приведёт к выходу нейрона\r\nблизкому к единице, а крестик - к нулю. \r\nОтношение <b class=\"norm\">w0/</b>|<b>&omega;</b>| равно расстоянию от плоскости \r\nдо начала координат <b class=\"norm\">(0,...,0)</b>.\r\nПо модулю оно не должно превышать <b class=\"norm\">n<sup>&frac12;</sup></b>\r\n\r\n<blockquote class=\"s3def\" >\r\n1. Нейрон является гиперплоскостью. Значение его выхода равно нормированному расстоянию от вектора входов до  гиперплоскости.\r\nВ процессе обучения, плоскость каждого нейрона меняет свою ориентацию и сдвигается в пространстве признаков.\r\n</blockquote>\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"sharp\"></a>\r\n<h2>Уверенность нейрона</h2>\r\n\r\n<p>\r\nПри обучении  сети необходим критерий, в соответствии с которым\r\nподбираются параметры нейронов. Обычно для этого служит квадрат отклонения\r\nвыходов сети от их целевых значений. Так, для двух классов и одного выхода,\r\nошибкой <b class=\"norm\">Error</b>\r\nсети считаем \r\n</p>\r\n<center style=\"\">\r\n<b class=\"norm\">\r\nError<sup>2</sup> = (1/N) <span style=\"font-size:24px;\">&sum;</span> (y-y<sub>c</sub>)<sup>2</sup>\r\n</b>,\r\n</center>\r\n<p>\r\nгде <b class=\"norm\">y</b>  - фактический  выход, а <b class=\"norm\">y<sub>c</sub></b> - его правильное\r\nзначение, равное <b class=\"norm\">0</b> для одного класса и <b class=\"norm\">1</b> - для второго.\r\nСумма ведётся по <b class=\"norm\">N</b> обучающим примерам.\r\n<a id=\"sqrError\"></a>\r\nЭту <i>среднеквадратичную ошибку</i>\r\n по всем обучающим объектам стараются сделать минимальной. \r\n Методы минимизации ошибки (подбор параметров нейронов) \r\n <a href=\"/lessons/4\">мы обсудим позднее</a>.\r\n</p>\r\n\r\n<a id=\"example01\"></a>\r\n<p>\r\nРассмотрим 2-мерное пространство признаков <b class=\"norm\">x<sub>1</sub>,x<sub>2</sub></b> и два класса\r\n<b class=\"norm\">0</b> и <b class=\"norm\">1</b>. На рисунке ниже объекты одного класса \r\nпредставлены синими кружочками, а объекты второго класса - красными крестиками.\r\nСправа от пространства признаков приведена сеть <b class=\"norm\">[2,1]</b> из одного нейрона.\r\nЗа ней, на  сине-красном квадратике, нарисована <i>карта значений</i> выхода нейрона при тех или иных  входах\r\n(<b class=\"norm\">x<sub>1</sub>,x<sub>2</sub></b> пробегают значения от <b class=\"norm\">0</b> до <b class=\"norm\">1</b>\r\nс шагом <b class=\"norm\">0.01</b>).\r\nЕсли <b class=\"norm\">y=0</b> - то это синий цвет, если \r\n<b class=\"norm\">y=1</b>\r\n- то красный, а белый цвет соответствует значению <b class=\"norm\">y=0.5</b>:\r\n</p>\r\n<b class=\"norm\">&sigma;=0.2D</b>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example01.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nСправа от рисунков под чертой, в квадратных скобках даны параметры  нейрона: <b class=\"norm\">[w<sub>0</sub>,w<sub>1</sub>,w<sub>2</sub>]</b>.\r\nВ круглых скобках приведена  длина <b class=\"norm\">|w|</b> вектора нормали <b>&omega;</b> \r\nи среднее значение выхода <b class=\"norm\">&lt;y&gt;</b> и его волатильность <b class=\"norm\">&sigma;<sub>y</sub></b> (см. ниже).\r\nОсь <b class=\"norm\">x<sub>1</sub></b> пространства признаков направлена вправо, а ось <b class=\"norm\">x<sub>2</sub></b>\r\n- вниз. Поэтому вектор  <b>&omega;</b> с положительными компонентами \r\n<b class=\"norm\">{w<sub>1</sub>, w<sub>2</sub>}</b>\r\n направлен по диагонали вниз (он нарисован рядом с кружочком\r\nна прямой, содержащим номер нейрона <b class=\"norm\">1</b>).\r\n</p>\r\n<p>\r\nНад чертой в таблице приведена среднеквадратичная ошибка <b class=\"norm\">Error</b>  такой сети.\r\nПри этом строка <b class=\"norm\">Learn</b> означает обучающую последовательность объектов, а \r\n<b class=\"norm\">Test</b> - проверочную,\r\nкоторая не участвовала в обучении \r\n(тестовые объекты на графике пространства признаков изображены полупрозрачными).\r\nКолонка <b class=\"norm\">Miss</b> содержит процент неправильно распознанных сетью объектов (отнесённых не к своему классу).\r\nПоследняя строка <b class=\"norm\">kNear</b> означает ошибку и процент ошибок \r\nв методе <b class=\"norm\">10</b>\r\nближайших соседей (он будет описан позднее).\r\n</p>\r\n<p>\r\nВ этом примере разброс признаков объектов каждого класса невелик.\r\nКлассы легко разделяются гиперплоскостью (линией в 2-мерии). \r\nСеть стремиться минимизировать ошибку к целевым \r\nзначениям <b class=\"norm\">0</b> или <b class=\"norm\">1</b>\r\nна выходе, поэтому модуль вектора <b class=\"norm\">|<b>w</b>|=48</b> принимает сравнительно большое значение.\r\nВ результате, даже недалеко расположенные от плоскости объекты (в обычном евклидовом смысле)\r\nполучают большое по модулю значение <b class=\"norm\">d</b>. Соответственно \r\n<b class=\"norm\">y=S(d) = 0</b> или <b class=\"norm\">1</b>.\r\nТакой нейрон  мы будем называть <i>уверенным</i>. Чем больше <b class=\"norm\">|<b>w</b>|</b>, тем более уверен в себе нейрон.\r\nНа его карте выхода тонкая белая линия (область неуверенности)\r\nотделяет насыщенный синий цвет (один класс) от насыщенного красного цвета (второй класс).\r\n</p>\r\n\r\n<p>\r\nНесколько иная ситуация во втором примере, где существует  широкая\r\nобласть перекрытия объектов различных классов.\r\nТеперь нейрон не столь уверен в себе и длина вектора <b class=\"norm\">|<b>w</b>|=24</b> в 2 раза меньше:\r\n</p>\r\n<b class=\"norm\">&sigma;=D</b>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example02.png\" style=\"width:950px;\">\r\n</center>\r\n<b class=\"norm\">&sigma;=D</b>\r\n<p>\r\nПриведём функции деформации расстояния (сигмоид) при длине вектора нормали,\r\nравной <b class=\"norm\">1,2,5,10,100</b>:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/sigmoids.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nТак как входы нейрона нормированы на единицу, максимальное расстояние от точки <b>x</b>\r\nс координатами <b class=\"norm\">{x<sub>1</sub>,...,x<sub>n</sub>}</b> в <b class=\"norm\">n</b>-мерном кубе (его диагональ)\r\nравна корню из <b class=\"norm\">n</b>. В 2-мерном пространстве признаков <b class=\"norm\">d<sub>max</sub>=1.4</b>.\r\nЕсли плоскость проходит через середину куба <b class=\"norm\">d<sub>max</sub>~0.5</b>.\r\n</p>\r\n\r\n<p>\r\nУверенный нейрон - не всегда хороший нейрон. Если размерность пространства признаков\r\n<b class=\"norm\">n</b> велика,\r\nа обучающих данных <b class=\"norm\">N</b>\r\nмало, сеть состоящая из уверенных нейронов может оказаться <i>переобученной</i>\r\nи на тестовых объектах приводить к большой ошибке распознавания. \r\nКроме этого самоуверенные нейроны  медленнее обучаются.\r\nПодробнее мы остановимся на этих вопросах ниже.\r\n</p>\r\n<p>\r\nВ заключение сформулируем главный вывод, справедливый для пространств любой размерности:\r\n</p>\r\n<blockquote class=\"s3def\" >\r\n2. Если два класса в пространстве признаков разделяются гиперплоскостью, то для \r\nих распознавания достаточно одного нейрона.\r\n</blockquote>\r\n\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"sharp2\"></a>\r\n<h2>Полезность нейрона</h2>\r\n\r\n<p>\r\nВ случае,\r\nесли гиперплоскость нейрона не пересекает единичный гиперкуб в котором\r\nнаходятся признаки (или значения выходов предыдущих нейронов), то \r\nот такого нейрона обычно мало пользы.\r\nОн не  разделяет на две части входные данные (которые всегда принадлежат\r\nинтервалу <b class=\"norm\">[0 ... 1]</b>. \r\nТакой нейрон будет называться <i>бесполезным</i>.\r\n</p>\r\n<p>\r\nНеобходимо стремиться к тому, чтобы все нейроны сети были полезными.\r\nИногда бесполезность появляется и для плоскости, пересекающей гиперкуб,\r\nесли объекты любых классов оказываются с одной стороны этой плоскости.\r\n</p>\r\n<p>\r\nПеред началом обучения параметры нейронов полагают равными случайным значениям.\r\nПри этом нейрон может сразу оказаться бесполезным.\r\nЧтобы этого не произошло, можно использовать следующий алгоритм инициализации:\r\n</p>\r\n<p style=\"margin-left:25px; border-left: 2px solid green; padding-left:5px;\">\r\nКомпоненты вектора <b>&omega;</b> задаём случайным образом, например в диапазоне <b class=\"norm\">[-w ... w]</b>,\r\nгде <b class=\"norm\">w ~ 1 - 10</b>.\r\nЗатем, внутри единичного гиперкуба (или в некоторой его центральной части), выбираем\r\nслучайную точку \r\n<b>x</b><b class=\"norm\"><sub>0</sub>={x<sub>01</sub>,...,x<sub>0n</sub>}</b>.\r\nПараметр сдвига задаём следующим образом:\r\n<b class=\"norm\">\r\nw<sub>0</sub> = -<b>&omega;&middot;x</b><sub>0</sub> = -(w<sub>1</sub>x<sub>01</sub> + ... + w<sub>n</sub>x<sub>0n</sub>)</b>.\r\nВ результате гиперплоскость будет гарантированно проходить через гиперкуб.\r\n</p>\r\n<p>\r\nПараметр сдвига стоит контролировать и в процессе обучения,\r\nтак чтобы нейрон был всё время полезным.\r\nЗдесь возможны два способа - геометрический и эмпирический. \r\nВ эмпирическом вычисляется среднее значение выхода каждого нейрона по обучающим объектам.\r\nЕсли после прохождения через сеть всех обучающих примеров, средние значения <b class=\"norm\">&lt;y&gt;</b>\r\nнекоторых нейронов близки к нулю или единице, то они считаются бесполезными.\r\nВ этом случае их можно \"встряхнуть\" случайным образом (возможно с сохранением вектора <b>&omega;</b>,\r\nизменяя только параметр сдвига <b class=\"norm\">w<sub>0</sub></b>).\r\n</p>\r\n<p>\r\nНа всех примерах в этом документе нейроны в сетях разукрашены в соответствии со значением\r\nих <b class=\"compress\">&lt;y&gt;</b>. Если <b class=\"compress\">&lt;y&gt;</b><b class=\"norm\"> = 0.5</b>, то нейрон белый,\r\nесли <b class=\"compress\">&lt;y&gt;</b><b class=\"norm\"> = 0</b> - синий, а если \r\n<b class=\"compress\">&lt;y&gt;</b><b class=\"norm\"> = 1</b> - красный.\r\nНасыщенный синий или красный цвета означают бесполезность нейрона.\r\nВ первых двух примерах, единственный нейрон получился очень полезным (белым), так как объекты\r\nклассов равновероятно находились справа и слева от линии (разделяющей гиперплоскости).\r\n</p>\r\n<p>\r\nКроме среднего значения выхода, важную роль играет <i>волатильность нейрона</i> &sigma;<sub>y</sub>,\r\nравная среднеквадратичным отклонениям его выхода от среднего значения\r\n<b class=\"compress\">&lt;y&gt;</b>.\r\nЧем волатильность меньше, тем \r\nменее полезен нейрон. Действительно, в этом случае, не зависимо от значений входов,\r\nон принимает одно и то же значение на выходе. Поэтому, без изменения выходных значений сети,\r\nтакой нейрон можно выбросить, сдвинув соответствующим образом параметры \r\nнейронов, для которых бесполезный нейрон является входным.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"outs3\"></a>\r\n<h2>Несколько выходов</h2>\r\n\r\n<p>\r\nРассмотрим теперь  <b class=\"norm\">3</b> класса. Использовать один нейрон не очень удобно,\r\nпоэтому, как было описано \r\n<a href=\"#box\">в начале документа</a>, создадим сеть <b class=\"norm\">[2,3]</b> с тремя выходами.\r\nПусть классы локализованы в пространстве признаков следующим образом:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example03.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nКаждый выходной нейрон отделяет \"свой класс\" от остальных двух.\r\nНапример, первый сверху нейрон (на рисунке горизонтальная плоскость номер <b class=\"norm\">1</b>)\r\nраспознаёт объекты, помеченные синими кружочками, выдавая на выходе <b class=\"norm\">1</b>,\r\nесли объект находится с той стороны, куда направлен вектор <b>&omega;</b> (чёрточка рядом с номером плоскости).\r\n</p>\r\n<p>\r\nАналогично, второй нейрон распознаёт красные крестики, а третий - зелёные квадратики.\r\nВ каждом случае векторы нормали направлены в сторону \"своих\" классов.\r\nВсе нейроны сети достаточно уверенны в себе и вполне полезны.\r\nИх небольшая синева связана с тем, что против вектора нормали всегда расположено вдвое больше данных (\"чужих\" двух классов),\r\nчем по вектору. Поэтому среднее значение каждого выхода смещено ниже нейтрального уровня <b class=\"norm\">0.5</b>.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"hide\"></a>\r\n<h2>Когда нужен скрытый слой</h2>\r\n\r\n<p>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_xor_01.png\" style=\"width:200px; float:right;  margin-left:2px;\">\r\nПерейдём теперь к чуть более сложной задаче. Пусть объекты двух классов (кружочки и крестики) сосредоточены по углам\r\nпространства признаков так, как  на рисунке справа. Одной гиперплоскостью (линией) эти два класса\r\nразделить нельзя. Иногда такую задачу называют разделяющим ИЛИ (<b class=\"norm\">xor</b>).\r\nЭта логическая операция равна  истине (единице) только, если один из аргументов равен истине, а второй лжи (нулю):\r\n\"Маша любит или Колю или Васю, но не их обоих\". На рисунке классу, \r\nпомеченными кружками, на выходе сеть должна выдавать ноль, а классу крестика - единицу.\r\nЕсли объекты находятся точно в углах, то <b class=\"norm\">xor(0,0) = xor(1,1) = 0</b>\r\nи <b class=\"norm\">xor(0,1) = xor(1,0) = 1</b>.\r\n</p>\r\n<p>\r\nЧтобы провести классификацию, необходима нейронная сеть <b class=\"norm\">[2,2,1]</b>\r\n с одним скрытым слоем.\r\nНиже на первом графике (в осях признаков <b class=\"norm\">x<sub>1</sub></b> и <b class=\"norm\">x<sub>2</sub></b>)\r\nпоказаны две гиперплоскости (линии <b class=\"norm\">A</b> и <b class=\"norm\">B</b>). \r\nОни соответствуют двум скрытым нейронам <b class=\"norm\">A</b> и <b class=\"norm\">B</b>.\r\nЗначения выходов нейронов приведены на втором графике (оси <b class=\"norm\">y<sub>A</sub></b> и <b class=\"norm\">y<sub>B</sub></b>).\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_xor_02.png\" style=\"width:750px;\">\r\n</center>\r\n<p>\r\nОба крестика лежат по направлениям векторов нормалей плоскостей <b class=\"norm\">A</b> и <b class=\"norm\">B</b>. \r\nПоэтому расстояние от них к плоскостям  будет положительным и, если нейроны достаточно уверены в себе, \r\nна их выходах  <b class=\"norm\">y<sub>A</sub></b> и <b class=\"norm\">y<sub>B</sub></b> \r\nбудет получаться единица (нижний правый угол плоскости <b class=\"norm\">y<sub>A</sub></b>, <b class=\"norm\">y<sub>B</sub></b>).\r\n</p>\r\n<p>\r\nКружок с признаками <b class=\"norm\">(0,0)</b> из верхнего левого угла плоскости\r\n<b class=\"norm\">x<sub>1</sub></b>, <b class=\"norm\">x<sub>2</sub></b> даст на выходах нейронов \r\n<b class=\"norm\">y<sub>A</sub>~0</b>, <b class=\"norm\">y<sub>B</sub>~1</b> (этот объект лежит против\r\nвектора нормали плоскости <b class=\"norm\">A</b> и по вектору нормали плоскости <b class=\"norm\">B</b>).\r\nВторой кружок с признаками <b class=\"norm\">(1,1)</b> даст на выходах нейронов значения \r\n<b class=\"norm\">y<sub>A</sub>~1</b>, <b class=\"norm\">y<sub>B</sub>~0</b>.\r\nПолучившееся \"деформированное\" пространство признаков \r\n<b class=\"norm\">y<sub>A</sub></b> и <b class=\"norm\">y<sub>B</sub></b> (второй график)\r\nуже легко разделить одной плоскостью <b class=\"norm\">C</b>, которая и будет выходным нейроном сети.\r\nЕсли вектор её нормали направлен так как указано на втором графике, то для обоих крестиков \r\nполучится <b class=\"norm\">y~1</b>, а для кружков <b class=\"norm\">y~0</b>.\r\n</p>\r\n\r\n<p>\r\nНиже приведен реальный пример нейронной сети, обученной распознавать два класса объектов, \r\nкаждый из которых разбивается на два кластера:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example04.png\" style=\"width:950px;\">\r\n</center>\r\n\r\n<blockquote class=\"s3def\" >\r\n3. Каждый слой  сети преобразует входное пространство признаков в некоторое другое пространство,\r\nвозможно, с иной размерностью. Такое нелинейное преобразование происходит до тех пор, пока\r\nклассы не оказываются линейно разделимыми нейронами выходного слоя.\r\n</blockquote>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"logic\"></a>\r\n<h2>Нейроны как логические элементы</h2>\r\n\r\n\r\n<p>\r\nК анализу поведения нейронов можно подойти с позиций математической логики.\r\nДля этого сконцентрируемся на одном классе задачи xor, например на крестиках.\r\nЗапишем логическое условие которому удовлетворяют все объекты этого класса. \r\nВ примере выше: \"любой крестик лежит по вектору плоскости <b class=\"norm\">A</b>\r\nи по вектору плоскости <b class=\"norm\">B</b>\".\r\nЭто кратко можно выразить формулой <b class=\"norm\">A & B</b>.\r\nВыходной нейрон \"<b class=\"norm\">C</b>\" реализует такое логическое \"И\".\r\nДействительно, его плоскость прижата к правому нижнему углу квадрата в пространстве признаков с координатами \r\n<b class=\"norm\">(1,1)</b>.\r\nПоэтому для входов (поставляемых нейронами \"<b class=\"norm\">A</b>\" и \"<b class=\"norm\">B</b>\")\r\nблизких к единице,\r\nна выходе этого нейрона будет \r\n<b class=\"norm\">1</b> (точнее его значение больше <b class=\"norm\">0.5</b>).\r\nПоэтому, как и положено, <b class=\"norm\">1 & 1 = 1</b>.\r\nЕсли хотя бы один из входов отличен от <b class=\"norm\">1</b>, то и выход будет нулевым \r\n(меньшим <b class=\"norm\">0.5</b>). Это справедливо и в пространстве произвольной\r\nразмерности, где гиперплоскость нейрона, обеспечивающего логическое \"И\" прижата \r\nк углу гиперкуба с координатами <b class=\"norm\">(1,1,...,1)</b> (отсекает его от остального гиперкуба).\r\n</p>\r\n\r\n<p>\r\nЕсли плоскость сместить в угол <b class=\"norm\">(0,0)</b>, сохранив направление нормали к углу <b class=\"norm\">(1,1)</b>,\r\nто такой нейрон будет логическим \"ИЛИ\". Его функция <b class=\"norm\">y=S(x<sub>1</sub>,x<sub>2</sub>)</b>\r\n даёт <b class=\"norm\">S(0,0)=0</b> и  в остальных случаях <b class=\"norm\">1</b> (ниже первый рисунок):\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/logic1.png\" style=\"width:750px;\">\r\n</center>\r\n<p>\r\nВ общем случае, плоскость нейрона, реализующего логическое \"ИЛИ\" отсекает угол <b class=\"norm\">(0,0,...,0)</b>\r\n<b class=\"norm\">n</b>-мерного куба, а вектор\r\nего нормали  направлен в сторону большего объём гиперкуба.\r\nВ отличии от этого, стандартное логическое \"И\" (второй рисунок) имеет вектор нормали в сторону меньшего объёма.\r\n</p>\r\n<p>\r\nЛогическое \"И\" для нейрона с <b class=\"norm\">n</b> входами\r\nописывается следующей функцией: \r\n</p>\r\n\r\n<center>\r\n<b class=\"norm\">y = S( w&middot;(x<sub>1</sub>+...+x<sub>n</sub>+&alpha;-n) )</b>,&emsp;&emsp;&emsp;&emsp;&emsp; <b class=\"norm\">y = x<sub>1</sub> & x<sub>2</sub> & ... & x<sub>n</sub></b>,\r\n</center>\r\n<p>\r\nгде параметр &alpha; - параметр, лежащий в диапазоне <b class=\"norm\">0&lt;&alpha;&lt;1</b>. \r\nЧем он ближе к нулю, тем сильнее\r\nплоскость прижата к углу с координатами <b class=\"norm\">(1,1,...,1)</b>. \r\nДействительно, когда <b class=\"norm\">&alpha;=0</b> и <b class=\"norm\">x<sub>1</sub>=...=x<sub>n</sub>=1</b>,\r\nимеем <b class=\"norm\">x<sub>1</sub>+...+x<sub>n</sub>-n=0</b>.\r\nЧтобы этот нейрон обеспечивал логическое И, он должен давать отрицательное расстояние к\r\n\"ближайшему\" углу гиперкуба у которого одна координата равна нулю: <b class=\"norm\">(1,...,1,0,1,...,1)</b>.\r\nЭто даёт ограничение <b class=\"norm\">&alpha;&lt;1</b>.\r\nОбщий множитель <b class=\"norm\">w</b>\r\nхарактеризует длину нормали (чем он больше, тем более уверен в себе нейрон). \r\n<a href=\"#neuron\">Сигмоидная функция</a>  <b class=\"norm\">S(d)</b> приведена в начале документа.\r\n</p>\r\n<p>\r\nАналогично записывается функция логического \"ИЛИ\" (<b class=\"norm\">0&lt;&alpha;&lt;1</b>)\r\n</p>\r\n</p>\r\n<center>\r\n<b class=\"norm\">y = S( w&middot;(x<sub>1</sub>+...+x<sub>n</sub>-&alpha;) )</b>,&emsp;&emsp;&emsp;&emsp;&emsp;<b class=\"norm\">y = x<sub>1</sub> &or; x<sub>2</sub> &or; ... &or; x<sub>n</sub></b>.\r\n</center>\r\n<p>\r\n<p>\r\nЕщё одна логическая функция отрицания реализуется при помощи вычитания.\r\nОбозначим её чертой над именем переменной. Тогда <b class=\"norm overline\">x</b> = <b class=\"norm\">1-x</b>\r\nи, как обычно, <b class=\"norm overline\">0</b><b class=\"norm\">=1</b>, <b class=\"norm overline\">1</b><b class=\"norm\">=0</b>.\r\nЕсли один из входов нейронов имеет отрицание, то его функция выхода имеет вид:\r\n</p>\r\n<center>\r\n<b class=\"norm\">y = S( w&middot;(-x<sub>1</sub>+...+x<sub>n</sub>+&alpha;-n+1) )</b>,&emsp;&emsp;&emsp;&emsp;&emsp;<b class=\"norm\">y = <b class=\"norm overline\">x</b><sub>1</sub> & x<sub>2</sub> & ... & x<sub>n</sub></b>.\r\n</center>\r\n<p>\r\nТаким образом, одна из компонент вектора нормали меняет свой знак и плоскость нейрона сдвигается.\r\nВыше на третьем и четвёртом рисунках приведены различные отрицания перменных.\r\nСтоит в этих терминах получить логическое ИЛИ из логического И при помощи  правила де-Моргана: \r\n</p>\r\n<center>\r\n<b class=\"norm\">!(x<sub>1</sub> & x<sub>2</sub>) = <b class=\"norm overline\">x</b><sub>1</sub> &or; <b class=\"norm overline\">x</b><sub>2</sub></b>,\r\n</center>\r\n<p>\r\nгде восклицательный\r\nзнак - ещё один способ обозначения логического отрицания.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"xor2\"></a>\r\n<h2>Четыре варианта xor</h2>\r\n\r\n<p>\r\nВ качестве упражнения, стоит проанализировать 4 различных реализаций бинарной \r\nфункции <b class=\"norm\">xor</b> при помощи нейронных сетей:\r\n</p>\r\n\r\n<center>\r\n<img style=\"width:850px; margin-top:10px;\" src=\"https://synset.com/ai/ru/nn/im/xor_1_4.png\">\r\n\r\n<table class=\"centerTD\">\r\n<tr>\r\n<td width=\"225px\"><img src=\"https://synset.com/ai/ru/nn/im/xor_1_4a.png\" style=\"width:120px; margin-top:10px;\"></td>\r\n<td width=\"225px\"><img src=\"https://synset.com/ai/ru/nn/im/xor_1_4b.png\" style=\"width:120px; margin-top:10px;\"></td>\r\n<td width=\"225px\"><img src=\"https://synset.com/ai/ru/nn/im/xor_1_4c.png\" style=\"width:120px; margin-top:10px;\"></td>\r\n<td width=\"225px\"><img src=\"https://synset.com/ai/ru/nn/im/xor_1_4d.png\" style=\"width:120px; margin-top:10px;\"></td>\r\n</tr>\r\n</table>\r\n</center>\r\n\r\n<p>\r\nПоследний ряд картинок - это карты выхода реально обученных нейронных сетей,\r\nимеющих разделяющие плоскости  на первом скрытом слое такие, как приведено в первом ряду картинок.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"y_x\"></a>\r\n<h2>Аппроксимация функции  <b class=\"norm\">y=f(x)</b></h2>\r\n\r\n<p>\r\nПри помощи нейронной сети с одним входом, одним выходом и достаточно большим скрытым слоем,\r\nможно аппроксимировать любую функцию <b class=\"norm\">y=f(x)</b>. Для доказательства,\r\nсоздадим сначала сеть, которая на выходе даёт <b class=\"norm\">1</b>, если\r\nвход лежит в диапазоне <b class=\"norm\">[a...b]</b> и <b class=\"norm\">0</b> - в противном случае.\r\n</p>\r\n\r\n\r\n<p>\r\nПусть <b class=\"norm\">&sigma;(d) = S(&omega;&middot;d)</b> - сигмоидная функция,\r\nаргумент которой умножен на большое число \r\n<b class=\"norm\">&omega;</b>,\r\n так что получается прямоугольная ступенька.\r\nПри помощи двух таких ступенек, можно создать столбик единичной высоты:\r\n</p>\r\n<center>\r\n<img style=\"width:700px; margin-top:10px;\" src=\"https://synset.com/ai/ru/nn/im/y_x_sigmoid.png\">\r\n</center>\r\n<p>\r\nНормируем аппроксимируемую функцию <b class=\"norm\">y=f(x)</b> на интервал <b class=\"norm\">[0...1]</b>\r\nкак для её аргумента <b>x</b>, так и для значения\r\n<b class=\"norm\">y</b>. \r\nРазобъём диапазон изменения <b class=\"norm\">x=[0...1]</b>\r\nна большое число интервалов (не обязательно равных). На каждом интервале  функция должна меняется незначительно.\r\nНиже приведено два таких интервала:\r\n</p>\r\n<center>\r\n<img style=\"width:700px; margin-top:10px;\" src=\"https://synset.com/ai/ru/nn/im/y_x_end.png\">\r\n</center>\r\n<p>\r\nКаждая пара нейронов в скрытом слое реализует единичный столбик. Величина <b class=\"norm\">d</b>\r\nравна <b class=\"norm\">w<sub>1</sub></b>, если <b class=\"norm\">x&in;(a,b)</b>\r\nи <b class=\"norm\">w<sub>2</sub></b>, если <b class=\"norm\">x&in;(b,с)</b>.\r\nЕсли выходной нейрон является линейным сумматором, то можно положить <b class=\"norm\">w<sub>i</sub>=f<sub>i</sub></b>,\r\nгде <b class=\"norm\">f<sub>i</sub></b> - значения функции на каждом интервале.\r\nЕсли же выходной нейрон - обычный нелинейный элемент, то необходимо пересчитать веса  \r\n<b class=\"norm\">w<sub>i</sub></b> при помощи обратной к сигмоиду функции (последняя формула).\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"y_x_py\"></a>\r\n<h2>Аппроксимация функции на Phyton</h2>\r\n<p>\r\nНиже приведен код на  языке  Phyton, который аппроксимирует функцию <b class=\"norm\">y=sin(pi*x)</b>:\r\n</p>\r\n<pre class=\"brush: py\" >\r\nimport numpy as np                    # библиотека численных методов \r\nimport matplotlib.pyplot as plt       # библиотека рисования графиков\r\n        \r\ndef F(x):                             # эту функцию аппроксимируем\r\n    return np.sin(np.pi*x);\r\n\r\nn=10                                  # число интервалов\r\nx1 = np.arange(0,   1,     1/n)       # координаты левых границ\r\nx2 = np.arange(1/n, 1+1/n, 1/n)       # координаты правых границ\r\nprint(\"x1:\",x1,\"\\nx2:\",x2)            # выводим эти массивы\r\n\r\nf  = F( (x1+x2)/2 )                   # функция в середине интервала\r\nfi = np.log( f/(1-f) )                # обратные значения к сигмоиду\r\n\r\ndef S(z, z0, s):                      # сигмоид\r\n    return 1/(1+np.exp(-100*s*(z-z0)))\r\n\r\ndef Net(z):                           # выход сети\r\n    return 1/(1+np.exp(-np.dot(fi, S(z, x1, 1) + S(z, x2, -1) -1)))\r\n\r\nx = np.arange(0.01, 1, 0.01)          # массив x-ов\r\ny = [ Net(z)  for z in x ]            # массив y-ов (выход сети)\r\n\r\nplt.plot(x, y)                        # результаты работы\r\nplt.plot(x, F(x))                     # исходная функция\r\nplt.show()                            # показываем картинку\r\n</pre>\r\n<p>\r\nВ результате работы, при <b class=\"norm\">n=10</b> и <b class=\"norm\">n=100</b> получаются следующие результаты:\r\n</p>\r\n<img style=\"width:200px; margin-top:10px;  margin-left:20px;\" src=\"https://synset.com/ai/ru/nn/im/y_x_py1.png\">\r\n<img style=\"width:200px; margin-top:10px;  margin-left:50px;\" src=\"https://synset.com/ai/ru/nn/im/y_x_py2.png\">\r\n\r\n\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"logic2\"></a>\r\n<h2>Нечёткая логика</h2>\r\n\r\n<p>\r\nИнтерпретация нейронов как логических элементов в дальнейшем упростит анализ архитектуры\r\nнейронных сетей. Кроме этого, существует замечательная возможность реализовывать\r\nпри помощи нейронных сетей системы, оперирующие с нечёткой логикой.\r\nВ такой логике,  кроме истины (<b class=\"norm\">1</b>) и лжи (<b class=\"norm\">0</b>) \r\nсуществует непрерывный диапазон истинностных значение от <b class=\"norm\">0</b>\r\nдо <b class=\"norm\">1</b>:\r\n</p>\r\n<center style=\"margin-top:5px;\">\r\n<table class=\"pressTD border norm leftTD leftTH\">\r\n<tr><th>Сумка<br>симпотная</th><th>Дорогая</th><th>Красного<br>цвета</th><th>Такая есть<br>у подруги</th><th>Моя машина<br>красная</th><th>У меня<br>депрессия</th><th>Покупаю<br>эту сумку!</th></tr>\r\n<tr><th>x<sub>1</sub></th><th>x<sub>2</sub></th><th>x<sub>3</sub></th><th>x<sub>4</sub><th>x<sub>5</sub></th><th>x<sub>6</sub></th><th>y</th></th><tr>\r\n<tr><td>1.0</td><td>0.0</td><td>1.0</td> <td>1.0</td> <td>0.0</td><td>0.0</td><td>0.0</td></tr>\r\n<tr><td>1.0</td><td>1.0</td><td>1.0</td> <td>0.0</td> <td>1.0</td><td>0.0</td><td>1.0</td></tr>\r\n<tr><td>0.5</td><td>0.5</td><td>0.5</td> <td>0.5</td> <td>0.5</td><td>1.0</td><td>1.0</td></tr>\r\n</table>\r\n</center>\r\n<p>\r\nПусть свойствами объектов являются  бинарные значения (\"да\", \"нет\").\r\nА выход чёрного ящика <b class=\"norm\">y</b> может принимать непрерывные значения.\r\nВ ряде случаев они также равны <b class=\"norm\">0</b> (\"нет\") или <b class=\"norm\">1</b> (\"да\").\r\nОднако при некоторых входах, выход может  равняться, например, <b class=\"norm\">0.5</b> (\"возможно\").\r\nНейронная сеть, наученная на подобных примерах, будет выдавать как бинарные рекомендации,\r\nтак и выражать степень своей неуверенности.\r\n</p>\r\n<p>\r\nНиже приведены функции, которые принято использовать в нечёткой логике и\r\nих аппроксимация (задача регрессии!) при помощи нейронной сети:\r\n<center>\r\n<div style=\"border:green 2px solid;  border-radius: 10px; width:600px; padding:10px; margin-top:5px\">\r\n<table class=\"rightTH\">\r\n<tr><th class=\"green\">and(x<sub>1</sub>,x<sub>2</sub>)= </th><td class=\"green\">x<sub>1</sub>*x<sub>2</sub></td></tr>\r\n<tr><th class=\"green\">or(x<sub>1</sub>,x<sub>2</sub>)= </th><td class=\"green\">x<sub>1</sub>+x<sub>2</sub>-x<sub>1</sub>*x<sub>2</sub></td></tr>\r\n<tr><th class=\"green\">xor(x<sub>1</sub>,x<sub>2</sub>)= </th><td class=\"green\">or(and(x<sub>1</sub>,1-x<sub>2</sub>),and(1-x<sub>1</sub>,x<sub>2</sub>))</td></tr>\r\n</table>\r\n</div>\r\n</center>\r\n\r\n<center>\r\n<img style=\"width:600px; margin-top:10px;\" src=\"https://synset.com/ai/ru/nn/im/fuzzy.png\">\r\n</center>\r\n\r\n</p>\r\n\r\nМы вернёмся к нечёткой логике позднее.\r\n<hr>\r\n<p>\r\n<a href=\"https://synset.com/ai/ru/nn/NeuralNet_01_Intro.html\">Ссылка</a> на оригинал статьи.\r\n</p>', '2022-04-22 09:53:12', '2022-04-22 09:53:12');
INSERT INTO `lessons` (`id`, `title`, `content`, `created_at`, `updated_at`) VALUES
(2, 'Нейронные сети: 2. Разделяющие поверхности', '<a id=\"intro\"></a>\r\n<h2>Введение</h2>\r\n\r\n<p>\r\nВ <a href=\"/lessons/1\">предыдущем документе</a> была рассмотрена геометрическая интерпретация\r\nнейрона, как вычислителя нормированного расстояния от вектора, подаваемого на его вход, к гиперплоскости,\r\nориентация и положение которой определяется параметрами нейрона\r\n<b class=\"norm\">(w<sub>0</sub>,w<sub>1</sub>,...,w<sub>n</sub>)</b>.\r\nВыход нейрона будет больше <b class=\"norm\">0.5</b>, если вход лежит по вектору нормали \r\n<b>&omega;</b><b class=\"norm\">={w<sub>1</sub>,...,w<sub>n</sub>}</b>\r\nк гиперплоскости и\r\nменьше <b class=\"norm\">0.5</b>, если против.\r\n</p>\r\n<p>\r\nЕсли гиперплоскость нейрона c  <b class=\"norm\">n</b> входами отсекает один из углов <b class=\"norm\">n</b>-мерного\r\nгиперкуба, то такой нейрон работает как логический элемент.\r\nКогда его вектор нормали направлен в этот угол (меньший объём пространства), \r\nто это логическое \"И\" входных значений (или их отрицаний),\r\nа если в противоположную сторону (в больший объём), то это логическое \"ИЛИ\". \r\nТаким образом, нейронная сеть может вычислять любую логическую функцию.\r\nПодбором <a href=\"/lessons/1#sharp\">уверенности нейрона</a> можно получить как стандартные булевы результаты,\r\nтак и вычисления в рамках нечёткой логики.\r\n</p>\r\n<p>\r\nВ этом документе, на примере 2-мерного пространства признаков, изучается как нейронная сеть справляется с построением поверхности\r\n(в 2-мерии линии) которая отделяет различные классы. Нас будет интересовать выбор \r\nоптимальной архитектуры, которая позволяет решать данную задачу.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"hide_conv\"></a>\r\n<h2>Выпуклые области</h2>\r\n\r\n<p>\r\nПусть объекты, помеченные крестиками, в пространстве признаков занимают треугольную область,\r\nподобно примеру на рисунке ниже:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_tri_01.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nТакие объекты можно легко распознать при помощи сети <b class=\"norm\">[2,3,1]</b>\r\nс одним скрытым слоем, содержащем три нейрона.\r\nВыше векторы нормалей разделяющих плоскостей нейронов направлены внутрь треугольника.\r\nПоэтому все крестки в пространстве выходов скрытого слоя\r\n<b class=\"norm\">y<sub>1</sub>,y<sub>2</sub>,y<sub>3</sub></b>\r\nсобираются в окрестности угла единичного куба  с координатами <b class=\"norm\">(1,1,1)</b>.\r\n Соответственно, эту область несложно отсечь единственной разделяющей\r\n3-мерной плоскостью, роль которой выполняет выходной нейрон, реализующий логическое \"И\".\r\nЕсли через <b class=\"norm\">A</b> обозначить утверждение \"точка лежит по вектору нормали к плоскости нейрона <b class=\"norm\">A</b>\"\r\nи аналогично для <b class=\"norm\">B</b> и <b class=\"norm\">С</b>,\r\nто приведенная выше нейронная сеть вычисляет логическую функцию <b class=\"norm\">A & B & C</b>.\r\n</p>\r\n<p>\r\nТак как граница области изломана, три нейрона входного слоя достаточно <a href=\"/lessons/1#sharp\">уверены</a> в себе \r\n(длины векторов велики). Выходной нейрон\r\nимеет малое среднее значение выхода (<b class=\"norm\">0.11</b>), что связано с преобладанием\r\nобъектов класса <b class=\"norm\">0</b> (синие кружочки).\r\nВообще говоря, при обучении желательно иметь примерное одинаковое число объектов различных классов.\r\nЭто позволяет  лучше контролировать полезность нейронов. Выше на полезность выходного нейрона\r\nуказывает его относительно большая волатильность <b class=\"norm\">0.31</b>.\r\n</p>\r\n<p>\r\n<a id=\"example_circle\"></a>\r\nСети с небольшим числом нейронов способны также описывать гладкие поверхности, разделяющие классы.\r\nНиже объекты, помеченные крестиками находятся внутри круга радиуса <b class=\"norm\">0.25</b>.\r\nСеть <b class=\"norm\">[2,3,1]</b> с такой же архитектурой, как у треугольника,  генерит на выходе \r\nдостаточно аккуратный круг:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_circle_01.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nОбратим внимание, что нейроны скрытого слоя  в <b class=\"norm\">10</b> раз менее уверены\r\nв себе, чем в случае с треугольником. В результате этого их границы более размыты, что приводит к сглаживанию\r\nуглов треугольника.\r\nА вот выходной нейрон очень в себе уверен, что даёт чёткую границу между синей и красной областями.\r\n</p>\r\n<blockquote class=\"s3def\" >\r\n4. Если  в пространстве признаков существует <i>единственная</i>  область с\r\n<i>выпуклой</i> границей, содержащая\r\nобъекты данного класса, то они могут быть распознаны при помощи сети с одним скрытым слоем.\r\n</blockquote>\r\n<p>\r\nВ качестве полезного упражнения стоит поиграться с параметрами нейронов \r\nсети с архитектурой <b class=\"norm\">[2,H,1]</b>, где число нейронов <b class=\"norm\">H</b> скрытого слоя\r\nможно изменять. Плоскости нейронов всё время фиксированы вдоль сторон правильного <b class=\"norm\">H</b>-угольника\r\nрадиуса <b class=\"norm\">R=0.4</b> (его также можно изменять).\r\nПервый параметр <b class=\"norm\">w<sub>H</sub></b> определяет длину векторов нормали нейронов скрытого слоя.\r\nСледующий параметр  <b class=\"norm\">w</b> - это длина нормали выходного нейрона,\r\nа <b class=\"norm\">w<sub>0</sub></b> - его сдвиг (в процентах от длины диагонали квадрата).\r\nМеняя эти 4 параметра необходимо перейти от правильного \r\n<b class=\"norm\">H</b>-угольник к окружности.\r\n</p>\r\n\r\n\r\n<!---------------------------------------------------------------------------------------------------->\r\n<!---------------------------------------------------------------------------------------------------->\r\n<script type=\"application/javascript\"   src=\"https://synset.com/ai/_js/draw.js\"></script>       <!--   -->\r\n<script type=\"application/javascript\"   src=\"https://synset.com/ai/_js/NeuroNet.js\"></script>         <!-- нейронные сети  -->\r\n<table>\r\n<tr><td><b class=\"green\">Cкрытый слой:</b></td><td> <b class=\"norm\">w<sub>H</sub></b> - длина нормали; <b class=\"norm\">R</b> - радиус многоугольниа (<b class=\"norm\">w<sub>0</sub></b>);</td></tr>\r\n<tr><td><b class=\"green\">Выходной нейрон:</b></td><td> <b class=\"norm\">w</b> - длина нормали;          <b class=\"norm\">shift</b> - % пересечения диагонали;</td></tr>\r\n</table>\r\n<br>\r\n<center>\r\n<div style=\"width:900px;height:380px;  padding:10px;  border: 2px solid blue; background-color: #EEF;  border-radius: 10px;\">\r\n<div style=\"float:left; width:400px; border: solid black 0px; text-align: left; margin-right:20px;\">\r\n<table class=\"norm\" style=\"margin-left:0px; border: solid black 0px;\">\r\n<tr><th>H:</th>\r\n<td> \r\n<select name=\"selectMenu\" size=\"1\" onChange=\"createNet(this.value);\" style=\"font-size:14px; color:#069; font-weight:bold; width:3em;\">\r\n   <option value=\"0\" selected=\"selected\">3</option>\r\n   <option value=\"1\" >4</option>\r\n   <option value=\"2\" >5</option>   \r\n   <option value=\"3\" >6</option>\r\n   <option value=\"4\" >7</option>\r\n   <option value=\"5\" >8</option>\r\n</select>\r\n</td>\r\n<td style=\"width:10px;\"></td>\r\n</tr>\r\n</table>\r\n<table class=\"norm rightTH\" style=\"margin-left:0px; border: solid black 0px;\">\r\n<tr>\r\n<th>w<sub>H</sub>:</th>\r\n<td>\r\n <input type=\"range\" name=\"points\" value=\"100\" min=\"1\" max=\"200\" style=\"width:300px;\" onchange=\"document.getElementById(\'wHID\').innerHTML = this.value; changeNet();\">\r\n</td>\r\n<td id=\"wHID\">\r\n100\r\n</td>\r\n</tr>\r\n\r\n<tr>\r\n<th>R:</th>\r\n<td>\r\n <input type=\"range\" name=\"points\" value=\"40\" min=\"1\" max=\"100\" style=\"width:300px;\" onchange=\"document.getElementById(\'RadiusID\').innerHTML = this.value/100; changeNet();\">\r\n</td>\r\n<td id=\"RadiusID\">\r\n0.4\r\n</td>\r\n</tr>\r\n<th>w:</th>\r\n<td>\r\n <input type=\"range\" name=\"points\" value=\"40\" min=\"1\" max=\"200\" style=\"width:300px;\" onchange=\"document.getElementById(\'wOID\').innerHTML = this.value; changeNet();\">\r\n</td>\r\n<td id=\"wOID\">\r\n40\r\n</td>\r\n</tr>\r\n\r\n<tr>\r\n<th>shift:</th>\r\n<td>\r\n <input type=\"range\" name=\"points\" value=\"100\" min=\"1\" max=\"100\" style=\"width:300px;\" onchange=\"document.getElementById(\'w0OID\').innerHTML = this.value/100; changeNet();\">\r\n</td>\r\n<td id=\"w0OID\">\r\n1.0\r\n</td>\r\n</tr>\r\n</table>\r\n<br>\r\n<div class=\"green\" id=\"nnetID\"></div>\r\n</div>\r\n<div style=\"float:right;\">\r\n<canvas id=\"canvasOutputID\" width=\"450\" height=\"400\" style=\"border: 0px dashed black;\"></canvas>\r\n</div>\r\n<div style=\"clear:both\"></div>\r\n</div>\r\n</center>\r\n\r\n<script>\r\n\'use strict\';\r\nlet nnet;\r\nlet numAngles = 3;\r\nlet Radius    = 0.4;\r\nfunction createNet(id)\r\n{\r\n   numAngles = parseInt(id) + 3;\r\n   nnet = new NeuroNet([2,numAngles,1]);\r\n\r\n   nnet.img.dy = 45;\r\n   nnet.img.dx = 45;\r\n   nnet.img.outW = 300;\r\n   changeNet();\r\n}\r\n\r\nfunction changeNet()\r\n{\r\n   let R   = parseFloat(document.getElementById(\"RadiusID\").innerHTML);\r\n   let wH  = parseFloat(document.getElementById(\"wHID\").innerHTML);\r\n   let wO  = parseFloat(document.getElementById(\"wOID\").innerHTML);\r\n   let w0O = parseFloat(document.getElementById(\"w0OID\").innerHTML);\r\n      \r\n   let PI = Math.PI, phi0 = [-PI/2, -PI/4, -PI/2, 0, -PI/2, -PI/8][numAngles-3];\r\n   let x1 = 0.5 + R*Math.cos(phi0);\r\n   let y1 = 0.5 + R*Math.sin(phi0);\r\n   let points = [{x:0.5+(x1-0.5)*Radius/R,y:0.5+(y1-0.5)*Radius/R}];\r\n   for(let i=1; i <= numAngles; i++){                 // веса нейронов скрытого слоя\r\n      let x2 = 0.5 +R*Math.cos(phi0+2*PI*i/numAngles);\r\n      let y2 = 0.5 +R*Math.sin(phi0+2*PI*i/numAngles);\r\n      points.push({x:0.5+(x2-0.5)*Radius/R,y:0.5+(y2-0.5)*Radius/R});\r\n      let x0 = (x1+x2)/2, y0 = (y1+y2)/2;             // середина ребра\r\n      let wx = -(x0-0.5), wy = -(y0-0.5);             // нормаль к нему, к центру\r\n      let ww = Math.sqrt(wx*wx+wy*wy);                // длина нормали\r\n      let w = nnet.neurons[2+i].weights;\r\n      w[1] = wx*wH/ww; w[2] = wy*wH/ww;  w[0] = -(x0*w[1]+y0*w[2]);      \r\n      x1=x2; y1=y2;\r\n   }\r\n   \r\n   let w = nnet.neurons[3+numAngles].weights;         // веса выходного нейрона\r\n   for(let i=1; i <= numAngles; i++)\r\n      w[i] = wO/Math.sqrt(numAngles);\r\n   w[0] = -w0O*Math.sqrt(numAngles)*wO;\r\n      \r\n   nnet.drawNet   (\"canvasOutputID\");   \r\n   nnet.drawOuts2D(\"canvasOutputID\");\r\n   \r\n   let canvas = document.getElementById(\'canvasOutputID\');\r\n   let ctx = canvas.getContext(\'2d\');\r\n   ctx.lineWidth = 0.5;  \r\n   w = nnet.img.outW;\r\n   let x0 = nnet.img.shiftX + 3*nnet.img.r+2* nnet.img.dx;\r\n   let y0 = (canvas.height - w)*0.5;  \r\n   ctx.strokeStyle = \"black\"; \r\n   ctx.beginPath();  \r\n   ctx.arc(x0+w/2,  y0+w/2, Radius*w, 0, Math.PI*2, true);\r\n   ctx.closePath();\r\n   ctx.stroke();\r\n   \r\n   ctx.beginPath();  \r\n   ctx.moveTo(x0+points[0].x*w, y0+points[0].y*w);\r\n   for(let i=1; i < points.length; i++)\r\n      ctx.lineTo(x0+points[i].x*w, y0+points[i].y*w);\r\n   ctx.closePath();\r\n   ctx.stroke();\r\n   \r\n   document.getElementById(\"nnetID\").innerHTML =  nnet.getSTR(false,false);\r\n}\r\ncreateNet(\"0\");\r\n\r\n</script>\r\n<P>\r\n[Подсказки: <b class=\"norm\">(3:200,0.4,200,0.85); (3:7,0.55,200,0.75)</b>]\r\n</P>\r\n<!---------------------------------------------------------------------------------------------------->\r\n<!---------------------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"min\"></a>\r\n<h2>Проблема локальных минимумов</h2>\r\n\r\n<p>\r\nМинимум <a href=\"/lessons/1#sharp\">среднеквадратичной ошибки</a> при распознавании образов,\r\nвообще говоря, не является самоцелью. Чаще важнее минимизировать процент неправильной классификации (<b class=\"norm\">Miss</b>).\r\nОднако среднеквадратичная ошибка более гладкая (дифференцируемая) функция параметров нейронов,\r\nпоэтому в ряде алгоритмов оптимизации она  удобнее.\r\n</p>\r\n<p>\r\n<img style=\"float:right; margin: 5px; width:250px\" src=\"https://synset.com/ai/ru/nn/im/min1.png\">\r\nОсновная проблема минимизации некоторой величины - это локальные минимумы,\r\nв которых алгоритм может \"застревать\". \r\nСправа приведен  пример минимизации\r\nфункции одной переменной, имеющей один глобальный и несколько локальных минимумов.\r\nПри этом серединное \"плато\" с рябью мелких локальных минимумов наклонено вправо в направлении \r\nлокального минимума.\r\nЧтобы попасть в глобальный, (самый глубокий) минимум необходимо сначала \"вскарабкаться в гору\".\r\n</p>\r\n<p>\r\nВ многомерном случае ситуация намного сложнее, так как существует много различных путей по которым\r\nможно попасть в минимум функции. Естественно, локальные минимумы также сильно портят жизнь.\r\nЛюбой алгоритм поиска минимума локален. Он определяет направление движения, которое,\r\nиз-за сложного рельефа (рябь на поверхности), \r\nможет не иметь ни чего общего с реальным направлением к глобальному минимуму.\r\nКроме этого, обычно, выбирается шаг (скорость обучения), с которым происходит перемещение в пространстве \r\nпараметров.\r\nЕсли шаг велик (в <a href=\"NeuroNet2D.html\">он-лайн примерах</a> это параметр \r\n<b class=\"norm\">Grad</b>), то можно проскочить глобальный минимум. Если же он мал - застрять в локальном.\r\n</p>\r\n<p>\r\nВернёмся к   <a href=\"/lessons/1#example01\">первому примеру</a> из предыдущего документа.\r\nТам было два класса, которые   легко разделялись плоскостью.\r\nВ этой задаче локальных минимумов нет и минимизация ошибки тривиальна.\r\nЭто уже не так в <a href=\"#example_circle\">примере с кругом</a>, когда нейронов в скрытом слое \r\nдостаточно много. Например, для сети <b class=\"norm\">[2,5,1]</b>, имеющей 21 параметр [<b class=\"norm\">5&middot;(2+1)+1&middot;(5+1)</b>],\r\nглобальный минимум имеет вид:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_circle_02.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nОднако поймать такой минимум непросто. Обычно при обучении, сеть скатывается\r\nк конфигурации нейронов скрытого слоя, подобной следующему примеру:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_circle_03.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nВ этой сети  один нейрон (4-й) является бесполезными, что сразу видно по его среднему значению выхода.\r\nЭтот локальный минимум соответствует квадрату, который можно было бы описать\r\nсетью <b class=\"norm\">[2,4,1]</b>.\r\n</p>\r\n<p>\r\nЕщё один пример локального минимума квадрата\r\nимеет  нейрон (2-й) проходящий по границе круга и приносящий мало пользы.\r\n Хотя обнаружить это по среднему и волатильности нейрона уже нельзя:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_circle_04.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nЭтот локальный минимум имеет в <b class=\"norm\">20</b> раз большую ошибку, чем \r\nкогда плоскости нейронов образуют правильный пятиугольник.\r\nКроме локальных минимумов, соответствующих треугольнику и квадрату,\r\nсуществуют овраги, связанные с вращением этих многоугольников, в котором параметры нейронов меняются,\r\nа ошибка - практически нет.\r\n</p>\r\n<p>\r\nЕщё один интересный пример связан с квадратной границей между классами.\r\nГлобальный минимум для этой задачи выглядит следующим образом:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_square_02.png\" style=\"width:950px;\">\r\n</center>\r\n<p>\r\nОднако, его \"поймать\" очень не просто и  чаще  алгоритм \"проваливается\"\r\nв следующий локальный минимум:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_square_01.png\" style=\"width:950px;\">\r\n</center>\r\n\r\n<p>\r\nВпрочем, поиск глобального минимума не всегда является целью. \r\nБолее того, достижение такого минимума может оказаться даже вредным.\r\nРассмотрим этот вопрос чуть подробнее.\r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"overlearn\"></a>\r\n<h2>Переобученность</h2>\r\n\r\n<p>\r\nЧем больше нейронов и, следовательно, параметров, \r\nтем более сложные поверхности, разделяющие классы   можно описать при помощи нейронной сети.\r\nОднако тут возникает проблема. Иногда, после обучения, ошибка по обучающим данным оказывается \r\nмалой, а по тестовым, наоборот, большой.\r\nВ этом случае говорят, что наступила <b>переобученность</b> системы.\r\n<img style=\"float:right; margin: 5px;\" src=\"https://synset.com/ai/ru/nn/im/over_learn.png\">\r\nОбычно такое происходит в сильно зашумлённых данных.  \r\nКлассический пример в одномерном случае - это аппроксимация данных некоторой функцией\r\n<b class=\"norm\">y=f(x)</b>.\r\nСправа на картинке красная, волнистая кривая построена по зелёным точкам, которые были выбраны в качестве\r\nобучающих. Эта кривая максимально \"старательно\" проходит через все обучающие объекты и имеет\r\nна них нулевую ошибку. Однако на тестовых объектах (синие точки) лучше сработает \r\nболее гладкая, чёрная линия. Поэтому уменьшение числа нейронов и связей между ними необходимо не только чтобы \r\nуменьшить число параметров которые необходимо подбирать.\r\nЕсли без сильного увеличения ошибки, можно уменьшить число нейронов и связей между ними,\r\nэто необходимо делать.\r\nТакая сеть приводит к более гладкой разделящей поверхности \r\nи меньшей вероятности возникновения эффекта переобученности.\r\n</p>\r\n<p>\r\nВернёмся  к модельному примеру с кругом. Когда он аппроксимировался  треугольником (сеть \r\n<b class=\"norm\">[2,3,1]</b>) ошибка равнялась <b class=\"norm\">Error=0.0472</b>.\r\nДля пятиугольника она существенно меньше: <b class=\"norm\">Error=0.0047</b>.\r\nОднако в обоих  случаях ошибка по тестовым примерам была \r\nпримерно одинакова (<b class=\"norm\">Error~0.1</b>)\r\nи заметно больше, чем при обучении.\r\n</p>\r\n<p>\r\nОбщая рекомендация может быть такая.\r\nЧасть обучающего множества объектов выделяются в отдельную, <i>контрольную группу</i>.\r\nЭти объекты не участвуют в подстройке параметров нейронов\r\nи контролируют, имеет ли смысл использовать более сложную архитектуру сети.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"hide2\"></a>\r\n<h2>Когда необходимы два скрытых слоя</h2>\r\n\r\n<p>\r\nРассмотрим разделяющие поверхности для иногда которых не достаточно одного скрытого слоя.\r\nПусть один из классов занимает в пространстве область с вмятинами (т.е. разделяющая поверхность не выпуклая)\r\nподобно конверту с красными крестиками на рисунке ниже: \r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_envel_01.png\" style=\"width:950px;\">\r\n</center>\r\n\r\n<p>\r\nЧтобы очертить границы этого конверта необходимо <b class=\"norm\">5</b> плоскостей-нейронов первого\r\nскрытого слоя. Но при помощи только логического \"И\" описать условие попадания крестика\r\nвнутрь конверта нельзя. \r\n</p>\r\n\r\n<p>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_envel_02.png\" style=\"float:right; width:200px;\">\r\nЛюбую плоскую фигуру можно рабить на треугольники. В примере выше таких треугольников два.\r\nЧтобы проверить попадание объекта внутрь треугольника, необходимо три нейрона.\r\nСправа на рисунке это  линии <b class=\"norm\">A,B,C</b> и  <b class=\"norm\">С,D,E</b>.\r\nОбъект может находиться в любом из этих треугольников или в обоих сразу в области их пересечения.\r\nПоэтому логическое условие требует дополнительного логического \"ИЛИ\", которое мы обозначаем символом \"&or;\":\r\n</p>\r\n<center class=\"norm\">\r\n(A & B & C) &or; (C & D & E).\r\n</center>\r\n<p>\r\nОно  означает: \"или в треугольнике <b class=\"norm\">A,B,C</b> или в треугольнике <b class=\"norm\">С,D,E</b> или в обоих\".\r\nНиже на рисунке представлена оптимальная архитектура сети для этой задачи. \r\nНейроны первого скрытого слоя \r\nявляются плоскостями (прямыми), образующими стороны \r\n треугольников  <b class=\"norm\">A,B,C</b> и <b class=\"norm\">С,D,E</b>.\r\nДва нейрона \r\n<b class=\"norm\">F</b> и <b class=\"norm\">G</b>\r\nследующего слоя реализуют  логические \"И\" (точка попала в треугольник  <b class=\"norm\">A,B,C</b>,\r\nточка попала в треугольник  <b class=\"norm\">С,D,E</b>).\r\nНаконец, выходной нейрон обеспечивает логическое \"ИЛИ\":\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_envel_03.png\" style=\"width:500px;\">\r\n</center>\r\n<p>\r\nВ квадратах приведено положение плоскостей нейронов во втором и третьем скрытых слоях. При этом\r\nпервые два квадрата, на самом деле необходимо нарисовать как кубы (у нейронов три входа).\r\nВ этих кубах нейроны <b class=\"norm\">G</b> и <b class=\"norm\">F</b> отсекают угол с координатами <b class=\"norm\">(1,1,1)</b>\r\nи в этот же угол направляют вектора своих нормалей.\r\n</p>\r\n\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"hide3\"></a>\r\n<h2>Несвязные области</h2>\r\n\r\n<p>\r\nПриведём ещё  пример \"Пуговица\", в котором объекты, помеченные красными крестиками, находятся \r\nвнутри двух несвязных кругов. При этом они окружены объектами другого класса:\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/example_2tri_01.png\" style=\"width:950px;\">\r\n</center>\r\nКак и в случае с конвертом, не обязательно использовать полносвязную нейронную,\r\nтак как условие для описания дырок пуговицы можно реализовать\r\nлогическим выражением типа <b class=\"norm\">(A & B & C & D) &or; (C & D & E & F)</b>.\r\nСтоит нарисовать соответствующую архитектуру.\r\n\r\n<blockquote class=\"s3def\" >\r\n5. Любую поверхность в пространстве признаков (в т.ч. несвязную и вогнутую) всегда можно описать\r\nсетью с двумя скрытыми слоями.\r\n</blockquote>\r\n\r\n<p>\r\nПредлагается самостоятельно проанализировать два класса, объекты которых\r\nрасположены на двух спиралях. Записав соответствующее логическое выражение, необходимо\r\nпоказать что \"минимально-оптимальная\" архитектура нейронной сети, описывающая разделяющую поверхность\r\nдолжна быть такой как на рисунке ниже.\r\n</p>\r\n<center>\r\n<img src=\"https://synset.com/ai/ru/nn/im/spirals.png\" style=\"width:950px;\">\r\n</center>\r\n\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"hide3\"></a>\r\n<h2>Иногда достаточно и одного слоя</h2>\r\n\r\n<p>\r\n<img src=\"https://synset.com/ai/ru/nn/im/envelope_simple.png\" style=\"width:300px; float:right; margin:5px;\">\r\nЛюбопытно, что в приведенных выше примерах, на самом деле, достаточна нейронная сеть с одним скрытым слоем.\r\nДля конверта две вертикальные линии\r\nи одна горизонтальная  более значимы, так как против векторов их нормалей нет ни одного\r\nкрестика. Поэтому присвоим их выходам веса <b class=\"norm\">2</b>.\r\nНаклонные прямые менее значимы, так как крестики могут находиться с любой их стороны.\r\nИх выходам присвоим вес <b class=\"norm\">1</b>. \r\n<br><br>\r\nПусть  <b class=\"norm\">y<sub>A</sub>,...,y<sub>E</sub></b> - выходы скрытого слоя.\r\nВычислим сумму:\r\n</p>\r\n<center>\r\n<b class=\"norm\">d = y<sub>A</sub> + y<sub>E</sub> + 2 (y<sub>B</sub>+y<sub>C</sub>+y<sub>D</sub>)</b>,\r\n</center>\r\n<p>\r\nгде <b class=\"norm\">y<sub>A</sub>, y<sub>E</sub></b> - выходы нейронов наклонных линий.\r\nЗначения <b class=\"norm\">d</b> для различных областей приведены справа на рисунке.\r\nВне конверта находятся значения <b class=\"norm\">3,4,5,6</b>,\r\nа внутри конверта: <b class=\"norm\">7,8</b>. Понятно, что эти два класса легко разделить \r\nодним нейроном:\r\n</p>\r\n<center>\r\n<b class=\"norm\">d = S(100&middot;( d - 6.5) )</b>,\r\n</center>\r\n<p>\r\nгде множитель <b class=\"norm\">100</b> выбран для повышения уверенности выходного нейрона\r\n(сигмоидная функция - ступенька).\r\n</p>\r\n<p>\r\nАналогичным образом предлагается проанализировать пример \"Пуговица\",\r\nв котором менее значимыми для выделения крестиков являются две внутреннии вертикальные линии.\r\n</p>\r\n<blockquote class=\"s3def\" >\r\n6. Чем больше вес входа нейрона, тем более значим этот вход.\r\n</blockquote>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"notok\"></a>\r\n<h2>Бесполезные бывают полезными</h2>\r\n\r\n\r\n<p>\r\nРассмотрим 3 класса и сеть без скрытых слоёв. Она выдаёт большую ошибку на втором выходе (нейрон <b class=\"norm\">4</b>),\r\nно правильно классифицирует все объекты. Как её это удаётся?\r\n</p>\r\n<center>\r\n<img style=\"width:950px; margin-top:20px;\" src=\"https://synset.com/ai/ru/nn/im/example_3classes.png\">\r\n</center>\r\n\r\n<p>\r\nНейроны <b class=\"norm\">3,5</b> смотрят в сторону своих классов (синие круги, зелёные квадраты)\r\nи очень уверены в себе. Поэтому на их выходах будет  <b class=\"norm\">1</b> для своего\r\nкласса и  <b class=\"norm\">0</b> - для остальных.\r\nНейрон  <b class=\"norm\">4</b> практически всегда на выходе имеет  <b class=\"norm\">y=0.21</b>.\r\nПоэтому он даст максимальный голос для красных крестиков (т.к. остальные там равны нулю).\r\n</p>\r\n<hr>\r\n<p>\r\n<a href=\"https://synset.com/ai/ru/nn/NeuralNet_02_Surface.html\">Ссылка</a> на оригинал статьи.\r\n</p>', '2022-04-22 10:06:07', '2022-04-22 10:06:07'),
(3, 'Нейронные сети: 3. Многомерность', '<a id=\"intro\"></a>\r\n<h2>Введение</h2>\r\n\r\n<p>\r\nВ <a href=\"/lessons/1\">предыдущем документе</a> были рассмотрены основы нейронных сетей.\r\nКаждый нейрон сети интерпретировался как разделяющая гиперплоскость в многомерном пространстве входов.\r\nНабор таких плоскостей позволяет строить, по крайней мере в принципе,  \r\nсколь угодно сложные поверхности. Эти поверхности можно использовать для разделения объектов, \r\nпринадлежащих различным классам.\r\n</p>\r\n<p>\r\nВ целях наглядности, рассматривались примеры в которых объекты имели два признака.\r\nТакие двумерные задачи легко изображать на плоскости, а нейроны представлять прямыми линиями.\r\nВ этом документе мы перейдём к задачам в которых число признаков больше двух.\r\nСоответственно, можно уже рассмотреть практически более полезные задачи, чем рисование \r\nпри помощи нейронной сети треугольников и кружков. \r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"dim\"></a>\r\n<h2>Проклятие размерности</h2>\r\n\r\n<p>\r\nМы живём в 3-мерном пространстве. Оно для нас настолько привычно, что свойства  пространств\r\nбольшого числа измерений часто оказываются неожиданными.\r\nТем не менее именно с такими пространствами имеют дело при распознавании образов (классификации объектов).\r\nНапомним, что мы рассматриваем объекты, которые характеризуются <b class=\"norm\">n</b> вещественными числами.\r\nПосле соответствующей нормировки их можно считать принадлежащими интервалу  от <b class=\"norm\">0</b> до <b class=\"norm\">1</b>.\r\nТаким образом, каждый объект представим точкой, находящейся внутри <b class=\"norm\">n</b>-мерного единичного гиперкуба.\r\nРассмотрим некоторые математические аспекты многомерия.\r\n</p>\r\n\r\n<h3>Гиперкуб в <b class=\"norm\">n</b> измерениях имеет <b class=\"norm\"> 2<sup>n</sup></b> вершин</b></h3>\r\n\r\n\r\n<p>\r\n<img style=\"float:left; margin: 5px; margin-right:10px; width:175px\" src=\"https://synset.com/ai/ru/nn/im/cube2D.png\">\r\n<img style=\"float:right; margin: 5px; width:225px\" src=\"https://synset.com/ai/ru/nn/im/cube3D.png\">\r\nЕдиничный 2-мерный куб (квадрат) имеет <b class=\"norm\">4</b> вершины с координатами\r\n<b class=\"norm\">(0,0), (0,1), (1,0), (1,1)</b>.\r\nУ 3-мерного куба их уже 8.\r\nВ пространстве   <b class=\"norm\">n</b> измерений\r\nкаждая точка  характеризуется <b class=\"norm\">n</b> координатами:\r\n<b class=\"norm\">{x<sub>1</sub>,...,x<sub>n</sub>}</b>. Пусть одна вершина куба находится \r\nв начале координат: <b class=\"norm\">{0,0,...,0}</b> (<b class=\"norm\">n нулей</b>).\r\nТогда остальные вершины нумеруются  всеми возможными последовательностями\r\n <b class=\"norm\">n</b> нулей и единиц:\r\n<b class=\"norm\">{0,0,...,0}</b>, <b class=\"norm\">{1,0,...,0}</b>, ..., <b class=\"norm\">{1,1,...,1}</b>.\r\nСуществует <b class=\"norm\"> 2<sup>n</sup></b> двоичных чисел с <b class=\"norm\">n</b> разрядами\r\nи, соответственно, <b class=\"norm\"> 2<sup>n</sup></b> вершин гиперкуба.\r\n</p>\r\n</p>\r\nПри больших <b class=\"norm\">n</b> гирперкуб очень \"колючий\" объект.\r\nНапример, в пространстве с 32 признаками он имеет <b class=\"norm\">4294967296</b> вершин.\r\n</p>\r\n<h3>\r\nМногомерное пространство <i>очень</i> большое.\r\n</h3>\r\n<p>\r\n<img style=\"float:right; margin: 5px; margin-right:10px; width:125px\" src=\"https://synset.com/ai/ru/nn/im/cube2Da.png\">\r\nПредположим, что мы хотим равномерно  заполнить пространство точками (обучающими объектами).\r\nДля этого каждый признак (ось) разобьём на <b class=\"norm\">k</b> равных частей.\r\nВ результате получится <b class=\"norm\">k<sup>n</sup></b> кубиков, в каждый из которых\r\nможно поместить точку. Понятно, что даже при <b class=\"norm\">k = 2</b>\r\nи <b class=\"norm\">n = 32</b> это сделать практически невозможно.\r\nПоэтому необходимо стремится к такому отбору признаков объектов, при котором соответствующие их\r\nклассам образы не  были бы слишком сложными. Иногда это формулируют в виде\r\n<i>гипотезы компактности</i>: каждый класс должен занимать относительно компактную область\r\nв <b class=\"norm\">n</b>-мерном пространстве и различные классы могут быть отделены друг\r\nот друга сравнительно простыми поверхностями. В противном случае задача не только распознавания,\r\nно и даже подготовки обучающего множества становится очень нетривиальной задачей.\r\n</p>\r\n\r\n\r\n<h3>\r\nШар\r\n</h3>\r\n\r\n<p>\r\n<img style=\"float:right; margin: 5px; margin-right:10px; width:150px\" src=\"https://synset.com/ai/ru/nn/im/eq_sphere_volume.png\">\r\nМножество равноудалённых точек от данной (центра) называется\r\nсферой. Пространство внутри сферы называется шаром.  Шар, вписанный в  гиперкуб это очень \"маленький\" объект.\r\nСправа приведена формула объёма шара в пространствах с чётным числом измерений\r\n<b class=\"norm\">n=2k</b> (чтобы не связываться с гамма-функцией). \r\nВ единичный гиперкуб можно вписать шар с  радиусом <b class=\"norm\">R=1/2</b>.\r\nВ соответствии с этой формулой его объём при <b class=\"norm\">n=10</b>\r\nравен <b class=\"norm\">V<sub>10</sub>=0.0025</b>, а при <b class=\"norm\">n=32</b> он исчезающе мал: <b class=\"norm\">V<sub>32</sub>=10<sup>-15</sup></b>.\r\nХотя шар \"прижимается\" к граням гриперкуба, его окружает очень много (<b class=\"norm\">2<sup>n</sup></b>) углов.\r\n</p>\r\n<p>\r\nПусть объекты данного класса характеризуются некоторыми типичными (средними)  \r\nзначениями признаков с небольшим разбросом вокруг этих средних значений.\r\nТогда образ этого класса является шаром в <b class=\"norm\">n</b>-мерном пространстве.\r\nЧем меньший разброс вокруг среднего, тем меньше радиус шара. Если у разных признаков разброс\r\nимеет различную амплитуду, то шар превращается в эллипсоид.\r\nЛинейным преобразованием всегда можно превратить эллипсоид в шар. Однако, если в прстранстве\r\nпризнаков существует несколько различных эллипсоидных кластеров объектов,\r\nпревратить в шар можно только один из них.\r\n</p>\r\n\r\n<h3>\r\nГиперплоскость\r\n</h3>\r\n\r\n<p>\r\n<i>Гиперплоскостью</i> в <b class=\"norm\">n</b>-мерном пространстве называют <b class=\"norm\">(n-1)</b>-мерное пространство\r\n(в 2-мерии это линия, а в 3-мерии \"обычная\" плоскость).\r\nГиперплоскость всегда можно провести через  <b class=\"norm\">n</b> точек. \r\nСоответственно, она задаётся при помощи  <b class=\"norm\">n</b> чисел (например,\r\nединичным вектором нормали (<b class=\"norm\"><b>&omega;</b><sup>2</sup>=1</b>) к плоскости\r\n<b>&omega;</b><b class=\"norm\">={w<sub>1</sub>,w<sub>2</sub>,...,w<sub>n</sub>}</b>\r\nи параметром сдвига <b class=\"norm\">w<sub>0</sub></b>). \r\n</p>\r\n<p>\r\nПусть есть два класса, объекты которых находятся внутри компактных сфер с достаточно большим расстоянием между ними.\r\nТогда для распознавания  \r\n<a href=\"/lessons/1#sharp\">достаточна сеть</a> только из одного нейрона. \r\nЕго плоскость отделит  кластеры друг от друга.\r\n</p>\r\n\r\n<h3>\r\nСимплекс\r\n</h3>\r\n\r\n<p>\r\n<img style=\"float:right; margin: 5px; margin-right:10px; width:200px\" src=\"https://synset.com/ai/ru/nn/im/tetra3D.png\">\r\n<i>Симплексом</i> называют <b class=\"norm\">n</b> мерное обобщение треугольника  2-мерного пространства.\r\nВ  3-мерии это тетраэдр (рисунок справа). \r\nЧерез <b class=\"norm\">n</b> точек в <b class=\"norm\">n</b>-мерном пространстве можно провести гиперплоскость.\r\nСоответственно <b class=\"norm\">n+1</b> точек в общем случае не лежат на одной гиперплоскости\r\nи образуют симплекс с <b class=\"norm\">n+1</b> вершинами. Если все рёбра (расстояния между парами вершин)\r\nодинаковы - симплекс называют правильным.\r\nНапротив каждой из <b class=\"norm\">n+1</b> вершин лежит гиперплоскость (в которую эта вершина не попала).\r\n</p>\r\n<p>\r\nПусть внутри сферы находятся объекты одного класса, а вне неё - объекты другого класса.\r\nДля их разделения \r\nпотребуется  нейронная сеть, имеющая\r\nпо меньшей мере <b class=\"norm\">n+1</b> скрытых слоёв. Они образуют \r\n<b class=\"norm\">n+1</b>  гиперплоскостей симплекса (возможно со сглаженными углами, если длины векторов\r\nнормали невелики).\r\n</p>\r\n<hr>\r\n<p>\r\n<a href=\"https://synset.com/ai/ru/nn/NeuralNet_03_Dimension.html\">Ссылка</a> на оригинал статьи.\r\n</p>', '2022-04-22 10:56:55', '2022-04-22 10:56:55'),
(4, 'Нейронные сети: 4. Обучение', '<script type=\"application/javascript\"   src=\"https://synset.com/ai/_js/menu.js\"></script>       <!-- меню -->\r\n\r\n<script type=\"text/javascript\" src=\"https://synset.com/_syntax/shCore.js\"></script>\r\n<script type=\"text/javascript\" src=\"https://synset.com/_syntax/shBrushJScript.js\"></script>\r\n<link href=\"https://synset.com/_syntax/shCore.css\" rel=\"stylesheet\" type=\"text/css\" />\r\n<link href=\"https://synset.com/_syntax/shThemeDefault.css\" rel=\"stylesheet\" type=\"text/css\" />\r\n<script type=\"text/javascript\">\r\n   SyntaxHighlighter.defaults[\'toolbar\'] = false;\r\n   SyntaxHighlighter.defaults[\'gutter\'] = false;\r\n   SyntaxHighlighter.all();\r\n</script>\r\n\r\n<script type=\"text/x-mathjax-config\"> MathJax.Hub.Config(mathjaxConfig); </script> <!-- mathjaxConfig определён в _js/menu.js -->\r\n<script type=\"text/javascript\" async  src=\"https://synset.com/_MathJax/MathJax.js?config=TeX-MML-AM_CHTML\"></script>\r\n\r\n<a id=\"intro\"></a>\r\n<h2>Введение</h2>\r\n<p>\r\nЭтот документ продолжает тему нейронных сетей.\r\nМы обсудим различные методы их обучения и трудности, возникающие при этом.\r\n</p>\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"nn0\"></a>\r\n<h2>Нейрон</h2>\r\n\r\nНапомним, что нейрон с $n$ входами характеризуется $n+1$ параметром: вектором нормали к разделяющей гиперплоскости\r\n$\\boldsymbol{\\omega}=\\{\\omega_1,...,\\omega_n\\}$ и её сдвигом $\\omega_0$.\r\nРасстояние $d$ от вектора входа $\\mathbf{x}$ к гиперплоскости равно:\r\n$$\r\nd=\\omega_0+\\omega_1x_1+...+\\omega_nx_n = \\omega_0+\\boldsymbol{\\omega}\\mathbf{x} = \\sum^{n}_{i=0}\\,\\omega_ix_i.\r\n$$\r\nДля сокращения выражений, удобно вводить фиктивный вход $x_0=1$, всегда равный единице.\r\nНа выходе  нейрона $y$ расстояние $d$ нормируется на диапазон $[0...1]$:\r\n$$\r\ny ~=~ y(\\omega,\\,\\mathbf{x}) ~=~ S(d) ~=~ S\\Bigr(\\sum^n_{i=0} \\omega_i x_i \\Bigr),~~~~~~~~~~~S(d) = \\frac{1}{1+e^{-d}}\r\n$$\r\nПроизводную сигмоидной функции можно  выразить через значение выхода нейрона:\r\n\\begin{equation}\\label{df_sigmoid}\r\nS\'(d) =  \\frac{e^{-d}}{(1+e^{-d})^2}=S(d)\\,\\bigr(1-S(d)\\bigr) = y\\,(1-y).\r\n\\end{equation}\r\nЕсли $y\\sim 0$ или $y\\sim 1$ производная $S\'(d)$ очень мала (горизонтальные участки сигмоиды).\r\nМаксимальное значение $1/4$\r\nпроизводной (наибольшая чувствительность нейрона)\r\nдостигается при при $y=1/2$ или $d=0$.\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"nn1\"></a>\r\n<h2>Однослойная сеть</h2>\r\n\r\n<p>\r\nПусть на вход нейрона подаётся вектор $\\mathbf{x}$, в результате чего на выходе получется\r\nзначение $y$. И пусть на самом деле значению $\\mathbf{x}$ должен соответствовать  выход $Y$.\r\nОпределим ошибку \"работы нейрона\", как среднеквадратичное отклонение желаемого выхода от фактического:\r\n$$\r\nE^2 = \\bigr(Y-y(\\omega,\\,\\mathbf{x})\\bigr)^2.\r\n$$\r\nЗадача обучения нейрона состоит в минимизации этой ошибки. Вычислим градиент ошибки.\r\nОн направлен в сторону её максимального роста (в обратном направлении ошибка убывает):\r\n$$\r\n\\frac{\\partial E^2}{\\partial \\omega_i} \r\n~=~ \\frac{\\partial E^2}{\\partial y}\\cdot\\frac{\\partial y}{\\partial d}\\cdot \\frac{\\partial d}{\\partial \\omega_i}\r\n~=~ \r\n-2\\,(Y-y)\\cdot y\\,(1-y)\\cdot x_i,\r\n$$\r\nгде учтена производная сигмоидной функции (\\ref{df_sigmoid}).\r\nТаким образом, вектор нормали $\\boldsymbol{\\omega}$ и смещение необходимо сдвинуть против градиента на величины:\r\n\\begin{equation}\r\n\\Delta\\boldsymbol{\\omega} = 2\\,\\gamma\\,(Y-y)\\,y\\,(1-y)\\,\\mathbf{x},~~~~~~~~~~~~~~~~\\Delta\\omega_0 = 2\\,\\gamma\\,(Y-y)\\,y\\,(1-y),\r\n\\end{equation}\r\nгде абсолютную величину сдвига (шаг)  задаёт параметр $\\gamma$.\r\n</p>\r\n<p>\r\n<img src=\"https://synset.com/ai/ru/nn/im/deltaW.png\" style=\"width:150px; float:right\">\r\nГеометрический смысл  этих формул прост. \r\nПусть $Y=1$, а нейрон выдаёт $y \\ll 1$. Тогда вектор нормали (и плоскость) повернётся в сторону\r\nпримера $\\mathbf{x}$, а $\\omega_0$ увеличится, т.е. плоскость сдвинется в противоположную от входа сторону.\r\nКогда $Y=0$, а $y \\sim 1$, множитель $Y-y$ становится отрицательным и всё произойдёт с точностью до наоборот.\r\n</p>\r\n<p>\r\nНа самом деле, множители $y\\cdot(1-y)$ вредят обучению. Например, пусть  $y\\sim 0$, а необходимо иметь \r\n$Y=1$. В этом случае сдвиг $\\omega_i$ должен быть существенным, а этого не происходит.\r\nБез изменения направления градиента, можно использовать другие формулы:\r\n$$\r\n\\Delta\\boldsymbol{\\omega} = \\gamma\\,f(Y-y)\\,\\mathbf{x},~~~~~~~~~~~~~~~~\\Delta\\omega_0 = \\gamma\\,f(Y-y),\r\n$$\r\nгде $f(z)$ - гладкая, нечётная функция, имеющая малые значения при $z\\sim 0$ (когда $Y\\approx y$)\r\nи большие по модулю при $z\\neq 0$. Вместо линейной зависимости, можно, например,\r\nвыбрать $(Y-y)^3$.\r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"nn1a\"></a>\r\n<h2>Усреднение градиента</h2>\r\n\r\n<p>\r\nМинимизируемая поверхность с кучей вмятинок будет приводить к \"дёрганью\" градиента.\r\nЧтобы уменьшить такой эффект, прежде чем изменять параметры нейрона $\\omega_i=\\{\\omega_0,\\boldsymbol{\\omega}\\}$,\r\nприращения $\\Delta\\omega_i$ усредняются по нескольким примерам. Такую \"пачку\" объектов называют bunch.\r\nВ банче могут быть, как примеры различных классов, так и одного. В последнем случае скорость передвижения\r\nвдоль градиента необходимо существенно уменьшить.\r\n</p>\r\n<p>\r\nЕщё один способ сглаживания градиента - это скользящее среднее. Получив очередное значение $\\Delta\\omega_i(t)$,\r\nвычисляется\r\n$$\r\n\\overline{\\Delta\\omega_i}(t) = \\beta\\cdot\\overline{\\Delta\\omega_i}(t-1)+(1-\\beta)\\cdot\\Delta\\omega_i(t).\r\n$$\r\nЕсли параметр $\\beta=0$, то усреднения нет. При $\\beta\\to 1$ происходит сильное усреднение.\r\nПараметры нейрона меняются по формулам:\r\n$$\r\n\\omega_i(t+1) = \\omega_i(t) - \\gamma\\cdot\\overline{\\Delta\\omega_i}(t).\r\n$$\r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"nn2\"></a>\r\n<h2>Контроль уверенности нейрона</h2>\r\n\r\nВ ряде случаев необходимо контролировать уверенность нейрона так, чтобы длина\r\nего вектора нормали $\\boldsymbol{\\omega}$ росла медленнее, чем ориентация и положение плоскости.\r\nДля этого воспользуемся методом множителей Лагранжа, введя дополнительные параметры\r\n$\\lambda$ и $N$:\r\n$$\r\nE^2 =\\bigr(Y-y(\\omega,\\,\\mathbf{x})\\bigr)^2 + \\lambda\\,\\bigr(\\boldsymbol{\\omega}^2-N\\bigr).\r\n$$\r\nВычислим градиент по всем параметрам:\r\n$$\r\n\\frac{\\partial E^2}{\\partial \\omega_i} = -2\\,(Y-y)\\,y\\,(1-y)\\,x_i + 2\\lambda\\,\\omega_i,\r\n~~~~~~~~~\r\n\\frac{\\partial E^2}{\\partial \\lambda} = \\boldsymbol{\\omega}^2-N,~~~~~~~~~\\frac{\\partial E^2}{\\partial N} = -\\lambda.\r\n$$\r\nСоответственно, в процессе обучения происходит подправка всех параметров по формулам:\r\n$$\r\n\\begin{array}{lcl}\r\n\\omega_i(t+1)&=&\\omega_i(t) - \\gamma_1\\cdot(-2\\,(Y-y)\\,y\\,(1-y)\\,x_i + 2\\lambda\\,\\omega_i),\\\\[2mm]\r\n\\lambda(t+1) &=& \\lambda(t) - \\gamma_2\\cdot(\\omega^2_1+...+\\omega^2_n-N),\\\\[2mm]\r\nN(t+1)       &=&  N(t) + \\gamma_3\\cdot\\lambda(t).\r\n\\end{array}\r\n$$\r\nЕстественно, для каждого нейрона множитель Лагранжа $\\lambda$ и целевая норма $n$\r\nдолжны быть свои (но одни и те же для всех весов нейрона $\\omega_i$)\r\n\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"nn3\"></a>\r\n<h2>Двухслойная сеть</h2>\r\n\r\n<p>\r\nПусть сеть имеет один скрытый слой и один выход. \r\nНа самом деле, любую сеть с $K$ выходами можно заменить на $K$\r\nсетей с одним выходом. Такие сети распознают (отличают) свой класс от <i>любых</i>\r\nдругих, т.е. строит отделяющую поверхность только для примеров своего класса.\r\nПонятно, что такая система имеет больше возможностей для подстройки параметров.\r\n</p>\r\n<p>\r\nВыход сети имеет вид:\r\n$$\r\ny = S\\Bigr(\\sum^n_{i=0} \\omega^{(2)}_{i}\\cdot S\\Bigr(\\sum^{n_I}_{j=0}\\omega^{(1)}_{ij}\\,x_j\\Bigr)\\,\\Bigr)\r\n=\r\nS\\Bigr( \\omega^{(2)}_{0} + \\omega^{(2)}_{1}\\, S\\Bigr(\\sum^{n_I}_{j=0}\\omega^{(1)}_{1j}\\,x_j\\Bigr)+...+\\omega^{(2)}_{n}\\, S\\Bigr(\\sum^{n_I}_{j=0}\\omega^{(1)}_{nj}\\,x_j\\Bigr)\\,\\Bigr),\r\n$$\r\nгде $n$ - число нейронов в скрытом слое, а $n_I$ - число входов сети.\r\nВычислим производную ошибки $E^2=(Y-y)^2$ по весам выходного нейрона:\r\n$$\r\n\\frac{\\partial E^2}{\\partial \\omega^{(2)}_i } = -2\\,(Y-y)~y\\,(1-y)~ y^{(1)}_i,\r\n$$\r\nгде $y^{(1)}_i$ - текущий выход $i$-того нейрона скрытого слоя и $y^{(1)}_0=1$.\r\nАналогично для производной по весам скрытого слоя:\r\n$$\r\n\\frac{\\partial E^2}{\\partial \\omega^{(1)}_{ij} } ~=~ -2\\,(Y-y)\\,y\\,(1-y)\\,\\omega^{(2)}_i\\,y^{(1)}_i\\,\\bigr(1-y^{(1)}_i\\bigr)\\,x_j\r\n~=~\r\n\\frac{\\partial E^2}{\\partial \\omega^{(2)}_i }\\,\\omega^{(2)}_i\\,\\bigr(1-y^{(1)}_i\\bigr)\\,x_j.\r\n$$\r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"nn4\"></a>\r\n<h2>Обратное распространение ошибки</h2>\r\n\r\n<p>\r\n<img style=\"float:right; margin: 5px; width:150px\" src=\"https://synset.com/ai/ru/nn/im/learn_02.png\">\r\nПусть $\\omega^\\alpha_{ij}$ - вес входа $i$-того нейрона в слое $\\alpha$, который получает \r\nсигнал от $j$-того нейрона предыдущего слоя $\\alpha-1$. \r\nОт весов $\\omega^\\alpha_{ij}$ непосредственно зависит выход нейрона $y^{\\alpha}_i$.\r\nПоэтому производная ошибки по весам (как сложная функция) имеет вид:\r\n$$\r\n\\frac{\\partial E^2}{\\partial \\omega^\\alpha_{ij}} \r\n~=~ \\frac{\\partial E^2}{\\partial y^\\alpha_i}\\,\\frac{\\partial y^\\alpha_i}{\\partial \\omega^\\alpha_{ij}}\r\n~=~ \\Bigr[\\frac{\\partial E^2}{\\partial y^\\alpha_i}\\,S\'(d^{\\alpha}_i)\\Bigr]~\\, y^{\\alpha-1}_j = \\chi^\\alpha_i\\,~ y^{\\alpha-1}_j.\r\n$$\r\nПри этом $i=1\\,...\\,n_\\alpha$ и $j=0\\,...\\,n_{\\alpha-1}$.\r\nВычислим производную ошибки $E$ по $y^\\alpha_i$. От выхода  $y^\\alpha_i$ непосредственно\r\nзависят выходы <i>всех</i> нейронов следующего слоя  $y^{\\alpha+1}_i$. Поэтому при взятии\r\nпроизводной необходимо поставить сумму:\r\n$$\r\n\\frac{\\partial E^2}{\\partial y^{\\alpha}_i}  \r\n~=~ \\sum^{n_{\\alpha+1}}_{j=1} \r\n\\frac{\\partial E^2}{\\partial y^{\\alpha+1}_j} \r\n\\frac{\\partial y^{\\alpha+1}_j}{\\partial y^\\alpha_i} \r\n~=~\r\n\\sum^{n_{\\alpha+1}}_{j=1} \r\n\\Bigr[\\frac{\\partial E^2}{\\partial y^{\\alpha+1}_j} \r\nS\'(d^{\\alpha+1}_j)\\Bigr]\\,\\omega^{\\alpha+1}_{ji}.\r\n$$\r\nУмножим левую и правую части на $S\'(d^\\alpha_i)=y^{\\alpha}_i\\,(1-y^{\\alpha}_i)$. Это даёт рекуррентную формулу\r\nдля определения коэффициентов $\\chi^\\alpha_i$:\r\n\\begin{equation}\\label{err_chi_rec}\r\n\\chi^\\alpha_i =y^{\\alpha}_i\\,(1-y^{\\alpha}_i)\\,\\sum^{n_{\\alpha+1}}_{j=1} \\chi^{\\alpha+1}_j\\,\\omega^{\\alpha+1}_{ji}.\r\n\\end{equation}\r\nТаким образом, алгоритм ообратного распространения ошибки имеет вид:\r\n<div style=\"border-left:solid blue 2px; margin-left:25px;\">\r\n<ul>\r\n<li> Подаём на вход пример $\\mathbf{x}$ и при прямом распространении вычисляем выходы всех нейронов\r\n$y^{\\alpha}_i$.\r\n<li> Для нейронов последнего слоя  $A$ вычисляем производные ошибок:\r\n$$\r\n\\chi^A_i ~=~ \\frac{\\partial E^2}{\\partial y^{A}_i}\\,S\'(d^A_i)~=~2\\,(Y_i-y^{A}_i)\\,y^{A}_i\\,(1-y^{A}_i).\r\n$$\r\n<li> С их помощью, по рекуррентным формулам (\\ref{err_chi_rec}) получаем коэффициенты $\\chi^\\alpha_i$ для всех нейронов\r\nсети.\r\n<li> Затем подправляем веса:\r\n$$\r\n\\Delta \\omega^{\\alpha}_{ji} = -\\gamma\\,\\chi^\\alpha_i\\,y^{\\alpha-1}_j.\r\n$$\r\n</ul>\r\n</div>\r\n</p>\r\n<!--------------------------------------------------------------------------------------->\r\n<hr>\r\n<a id=\"info\"></a>\r\n<h2>Информация</h2>\r\n\r\n<pre>\r\nОшибки I и II рода\r\n• Пусть, существует «основной класс»\r\n• Обычно, это класс, при обнаружении которого, предпринимается какое-либо действие;\r\n• Например, при постановке диагноза основным классом будет «болен», а вторичным классом «здоров».\r\n• Ошибка первого рода равна вероятности принять основной класс за вторичный\r\n• Вероятность «промаха», когда искомый объект будет пропущен\r\n• Ошибка второго рода равна вероятности принять вторичный класс за основной\r\n• Вероятность «ложной тревоги», когда за искомый объект будет принят «фон»\r\n\r\nROC – Receiver Operating Characteristic curve\r\n• Кривая, отражающая зависимость чувствительности и ошибки второго рода по сранению с монеткой\r\n• Площадь под графиком – AUC дает некоторый объективный показатель качества классификатора\r\n\r\nБутстраппинг (Bootstrapping)\r\n• Выбираем отрицательные примеры случайным образом\r\n• Обучаем классификатор\r\n• Применяем к данным\r\n• Добавляем ложные обнаружение к выборке\r\n• Повторяем\r\n• Смысл:\r\n• Соберем ограниченную, но представительную выборку «не-обьектов»\r\n• Заставим классификатор сконцентрироваться на сложных отрицательных (hard negative) примерах\r\n</pre>\r\n<hr>\r\n<p>\r\n<a href=\"https://synset.com/ai/ru/nn/NeuralNet_04_Learn.html\">Ссылка</a> на оригинал статьи.\r\n</p>', '2022-04-22 11:01:34', '2022-04-22 11:01:34');

-- --------------------------------------------------------

--
-- Структура таблицы `migrations`
--

CREATE TABLE `migrations` (
  `id` int(10) UNSIGNED NOT NULL,
  `migration` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `batch` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Дамп данных таблицы `migrations`
--

INSERT INTO `migrations` (`id`, `migration`, `batch`) VALUES
(1, '2014_10_12_000000_create_users_table', 1),
(2, '2014_10_12_100000_create_password_resets_table', 1),
(3, '2019_08_19_000000_create_failed_jobs_table', 1),
(4, '2019_12_14_000001_create_personal_access_tokens_table', 1),
(5, '2022_03_12_130058_create_lessons_table', 1),
(6, '2022_03_13_204546_create_recipes_table', 2),
(7, '2022_03_16_051816_create_users_table', 3),
(8, '2022_03_16_121918_create_users_table', 4),
(9, '2022_03_22_183929_create_questions_table', 5);

-- --------------------------------------------------------

--
-- Структура таблицы `password_resets`
--

CREATE TABLE `password_resets` (
  `email` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `token` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- Структура таблицы `personal_access_tokens`
--

CREATE TABLE `personal_access_tokens` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `tokenable_type` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `tokenable_id` bigint(20) UNSIGNED NOT NULL,
  `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `token` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL,
  `abilities` text COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `last_used_at` timestamp NULL DEFAULT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- Структура таблицы `questions`
--

CREATE TABLE `questions` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `title` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `answers` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Дамп данных таблицы `questions`
--

INSERT INTO `questions` (`id`, `title`, `answers`, `created_at`, `updated_at`) VALUES
(1, 'Тестовый вопрос 1', 'Вариант ответа 1;Вариант ответа 2;Вариант ответа 3', NULL, NULL),
(2, 'Тестовый вопрос 2', 'Вариант ответа 1;Вариант ответа 2;Вариант ответа 3', NULL, NULL);

-- --------------------------------------------------------

--
-- Структура таблицы `recipes`
--

CREATE TABLE `recipes` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `title` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `ingredients` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `description` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Дамп данных таблицы `recipes`
--

INSERT INTO `recipes` (`id`, `title`, `ingredients`, `description`, `created_at`, `updated_at`) VALUES
(1, 'Борщ', 'Говядина - 300 г\r\nКапуста белокочанная - 500 г\r\nСвёкла - 1-2 шт.\r\nКартофель - 3-4 шт.\r\nМорковь - 2 шт.\r\nЛук репчатый - 1 шт.\r\nТоматная паста - 2 ст. ложки\r\nМасло растительное - 4 ст. ложки\r\nЛавровый лист - 1-2 шт.\r\nПерец черный горошком - 2-3 шт.\r\nСоль - 2-3 ч. ложки (по вкусу)\r\nЛимонная кислота (по желанию) - на кончике ножа\r\nЧеснок - по вкусу\r\nЗелень - по вкусу\r\nСметана (для подачи) - по вкусу', '1. Подготовьте продукты. У меня среди ингредиентов нет томата, но в традиционный борщ, конечно, добавляется томатная паста и/или протертые помидоры.\r\n2. Мясо залейте в кастрюле водой (1,5-2 л), поставьте на огонь. Когда вода закипит, осторожно снимите накипь и оставьте говядину вариться на малом огне около 2 часов.\r\n3. Свеклу очистите, нарежьте соломкой.\r\n4. \r\nФото приготовления рецепта: Борщ «Традиционный» - шаг №4\r\n\r\nВ сковороде разогрейте 2 ст. ложки растительного масла, выложите свеклу и слегка обжарьте (5 минут), а затем залейте небольшим количеством бульона (0,5 стакана) и тушите около 20-25 минут, до мягкости.\r\n5. Картофель очистите, вымойте и нарежьте небольшими кубиками или крупной соломкой.\r\n6. Капусту нарежьте крупной соломкой.\r\n7. Когда мясо будет готово, добавьте капусту в бульон.\r\n(Мясо можно вынуть из бульона, отделить от костей, нарезать и добавить в борщ в конце приготовления.)\r\n8. Лук и морковь очистите, вымойте. Лук мелко нарежьте, морковь натрите на тёрке.\r\n9. В сковороде разогрейте 2 ст. ложки масла. Выложите лук и морковь и обжарьте, помешивая, на среднем огне минут 5, до золотистого цвета.\r\n10. Картофель и зажарку добавьте в бульон с капустой. Когда бульон снова закипит, добавьте соль и перец. Далее варите на малом огне 15-20 минут.\r\n11. Через 15-20 минут добавьте в борщ тушеную свеклу (и томатную пасту, если используется).\r\nВарите борщ ещё 5-7 минут.\r\nДля того чтобы свёкла сохранила красный цвет, можно всыпать в борщ щепотку лимонной кислоты. Это также улучшит вкус борща.\r\nЗа 3–5 минут до окончания варки добавьте лавровый лист. А после того как снимете с огня – пропущенный через пресс чеснок и измельченную зелень.\r\n12. Разлейте борщ по тарелкам, при желании дополнив кусочками мяса. Подавайте борщ со сметаной.\r\nПриятного аппетита!', '2022-03-13 21:31:17', '2022-03-13 21:31:17'),
(2, 'Яичница в хлебе, с сыром и колбасой', 'Яйца - 3 шт.\r\nХлеб белый - 100 г (3 ломтика)\r\nКолбаса варёная - 75 г\r\nСыр твёрдый - 30 г\r\nСоль - щепотка\r\nПерец чёрный молотый - щепотка\r\nМасло растительное (для жарки) - 1 ст. ложка', 'Подготовьте продукты по списку.\r\nЕсли у вас целый хлеб, отрежьте от него три ломтика толщиной 1,2-1,5 см.\r\nНачинка у нас будет колбасная, но можно добавить кубики свежего сладкого перца или зелёный горошек - это освежит и разнообразит вкус.\r\n\r\nКолбасу нарежьте мелкими кубиками. Сыр натрите на крупной тёрке.\r\n\r\nОстрым ножом удалите в ломтиках мякиш, оставив корочку шириной 0,8-1 см.\r\n\r\nВ сковороде на максимальном огне разогрейте растительное масло. Обжарьте колбасу, постоянно помешивая, примерно 3-4 минуты, до румяности. Затем выложите колбасу из сковороды на тарелку.\r\n\r\nВыложите в сковороду подготовленные ломтики хлеба. Обязательно уменьшите огонь до минимума.\r\n\r\nВнутрь ломтиков выложите кубики обжаренной колбасы.\r\n\r\nВ каждый ломтик хлеба аккуратно вбейте по одному яйцу. Будет красивее, если желток при этом останется целым. Посолите яйца.\r\n\r\nГотовьте яичницу в хлебе на минимальном огне примерно 6-7 минут, до схватывания белка.\r\n\r\nПосыпьте яичницы в хлебе натёртым сыром и накройте сковороду крышкой. Готовьте ещё примерно 1-1,5 минуты, до расплавления сыра.\r\n\r\nЯичница в хлебе, с сыром и колбасой, готова. Слегка присыпьте чёрным молотым перцем.\r\n\r\nПодавайте яичницу к столу, по желанию дополнив зеленью петрушки или укропа\r\n\r\nХрустящий ломтик хлеба, сочная жареная колбаска, упругий белок и растекающийся аппетитный желток - суперзавтрак для отличного начала дня!\r\n\r\nПриятного аппетита!', '2022-03-13 21:47:31', '2022-03-13 21:47:31'),
(9, 'Борщ украинский', 'Мясо (мякоть говядины) - 250 г\r\nСвёкла (среднего размера) - 1 шт.\r\nКапуста белокочанная - 150-180 г\r\nКартофель - 200 г\r\nМорковь - 1 шт.\r\nЛук репчатый (небольшого размера) - 2 шт.\r\nЧеснок - 1-2 зубчика\r\nТоматная паста - 1,5 ст. ложки\r\nУксус 6% - 1 ч. ложка\r\nМасло растительное - 30-40 г\r\nЛавровый лист - 1 шт.\r\nСоль - по вкусу\r\nПерец чёрный молотый - по вкусу\r\nВода - 2,5 л\r\nСметана (для подачи) - по вкусу\r\nУкроп свежий (для подачи) - по вкусу', 'Подготовить все необходимые ингредиенты.\r\nКартофель, морковь и свёклу почистить.\r\n\r\nМясо залить холодной водой, довести до кипения, снять пену. Варить бульон на медленном огне 50-60 минут, до мягкости мяса. Бульон посолить.\r\n\r\nКартошку нарезать брусочками.\r\n\r\nКапусту нарезать полосками.\r\n\r\nЛук очистить и нарезать кубиками.\r\nМорковь натереть на крупной тёрке.\r\n\r\nСвёклу нарезать небольшими брусочками.\r\n\r\nКартошку отправить в бульон с мясом и варить 7-10 минут на среднем огне.\r\n\r\nПоловину растительного масла налить в разогретую сковороду. Обжарить лук до прозрачности, примерно 3-4 минуты на среднем огне, помешивая.\r\n\r\nДобавить к луку морковь.\r\n\r\nОбжарить морковь с луком пару минут. Переложить зажарку из сковороды на тарелку.\r\n\r\nОбжарить свёклу на оставшемся масле в течение 2-3 минут.\r\n\r\nДобавить к свёкле томатную пасту, можно влить пару ложек воды. Влить уксус, благодаря которому свёкла сохранит свой яркий цвет и борщ приобретёт красивый бордовый оттенок.\r\n\r\nТушить свёклу 5 минут на умеренном огне.\r\n\r\nКогда картошка сварится, добавить в кастрюлю капусту и варить минут 5-7.\r\n(Молодую капусту добавлять в борщ в конце его приготовления и варить максимум минут пять, чтобы капуста не разварилась.)\r\n\r\nДобавить в кастрюлю тушёную свёклу. Варить борщ со свёклой 5-6 минут.\r\n\r\nЗатем добавить лук с морковью. Варить борщ еще 4-5 минут на небольшом огне.\r\n\r\nДобавить лавровый лист и немного чёрного молотого перца.\r\nЧеснок очистить и выдавить через пресс в кастрюлю с борщом. (По \"классике\" в украинский борщ добавляется чеснок, растертый с салом, солью и зеленью. При желании можно сделать именно так.)\r\nДовести борщ до кипения и сразу снять с огня.\r\nЗакрыть кастрюлю крышкой и дать борщу настояться 20-30 минут.\r\n\r\nГотовый красный борщ разлить по тарелкам, добавить сметану и посыпать рубленым укропом.\r\n\r\nПриятного аппетита!', '2022-03-14 00:56:39', '2022-03-14 00:56:39'),
(10, 'Салат \"Мимоза\" классический', 'Сардина в масле - 200 г\r\nЯйца - 6 шт.\r\nСыр - 150 г\r\nЛук - 70-100 г\r\nМасло сливочное замороженное - 100 г\r\nЗелень укропа - 0,25-0,5 пучка\r\nМайонез - 150 г', 'Как приготовить салат \"Мимоза\" классический:\r\n\r\nРыбу вынуть из масла. Отделить от крупных костей и размять вилкой.\r\n\r\nЯйца отварить, остудить, очистить. Отделить белки от желтков.\r\n\r\nЖелтки размять вилкой.\r\n\r\nБелки натереть на мелкой терке.\r\n\r\nСыр натереть на мелкой терке.\r\n\r\nЗелень мелко порубить.\r\n\r\nЛук очистить и натереть.\r\n\r\nКогда все ингредиенты подготовлены, выложить салат \"Мимоза\" классический слоями:\r\n1-й слой - яичные белки.\r\n\r\n2-й слой - сыр.\r\n\r\n3-й слой - рыба.\r\n\r\nСверху на рыбу нужно распределить 0,75 всего майонеза.\r\n\r\n4-й слой - лук.\r\n\r\n5-й слой - половина яичных желтков.\r\n\r\nЗатем оставшийся майонез.\r\n\r\n6-й слой - зелень. Так как я хотела добиться \"зимнего\" настроения, то взяла двойную порцию зелени и посыпала ею не только верх, но и бока салата.\r\n\r\n7-й слой - сливочное масло. Замороженное сливочное масло нужно натереть на мелкой терке прямо на центр салата, а затем аккуратно распределить его по всей поверхности.\r\n\r\nИ, наконец, финальный, 8-й слой - оставшиеся яичные желтки. Для большего эффекта рекомендую желтки дополнительно протереть через сито прямо на салат и аккуратно их распределить.\r\n\r\nСалат нужно поставить в холодильник на 2 часа.\r\n\r\nСалат \"Мимоза\" классический готов. Приятного аппетита!', '2022-03-14 01:01:28', '2022-03-14 01:01:28');

-- --------------------------------------------------------

--
-- Структура таблицы `users`
--

CREATE TABLE `users` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `email` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `is_admin` tinyint(1) NOT NULL DEFAULT 0,
  `password` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- Дамп данных таблицы `users`
--

INSERT INTO `users` (`id`, `name`, `email`, `is_admin`, `password`, `created_at`, `updated_at`) VALUES
(1, 'User', 'user@mail.ru', 0, '$2y$10$kzdTQlJcqm01wKfpov2oAe2QPi8ajrYMIF.4qZexwUmCnRBw0Vf9C', '2022-03-16 12:24:53', '2022-03-16 12:24:53'),
(2, 'Admin', 'admin@mail.ru', 1, '$2y$10$yqmuNNjX15G3OJje0IWsve0VKJ7HAJrcOPHZbHmpBKAeCAiEa7lJ.', '2022-03-16 12:25:46', '2022-03-16 12:25:46'),
(3, 'Константин', 'filippovk997@mail.ru', 0, '$2y$10$OlUnitPRFWM9WZI1le.ds.0uNzkg85dxa737WCM50eWmZPAtIQdwO', '2022-04-24 22:51:21', '2022-04-24 22:51:21');

--
-- Индексы сохранённых таблиц
--

--
-- Индексы таблицы `failed_jobs`
--
ALTER TABLE `failed_jobs`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `failed_jobs_uuid_unique` (`uuid`);

--
-- Индексы таблицы `lessons`
--
ALTER TABLE `lessons`
  ADD PRIMARY KEY (`id`);

--
-- Индексы таблицы `migrations`
--
ALTER TABLE `migrations`
  ADD PRIMARY KEY (`id`);

--
-- Индексы таблицы `password_resets`
--
ALTER TABLE `password_resets`
  ADD KEY `password_resets_email_index` (`email`);

--
-- Индексы таблицы `personal_access_tokens`
--
ALTER TABLE `personal_access_tokens`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `personal_access_tokens_token_unique` (`token`),
  ADD KEY `personal_access_tokens_tokenable_type_tokenable_id_index` (`tokenable_type`,`tokenable_id`);

--
-- Индексы таблицы `questions`
--
ALTER TABLE `questions`
  ADD PRIMARY KEY (`id`);

--
-- Индексы таблицы `recipes`
--
ALTER TABLE `recipes`
  ADD PRIMARY KEY (`id`);

--
-- Индексы таблицы `users`
--
ALTER TABLE `users`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `users_email_unique` (`email`);

--
-- AUTO_INCREMENT для сохранённых таблиц
--

--
-- AUTO_INCREMENT для таблицы `failed_jobs`
--
ALTER TABLE `failed_jobs`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT;

--
-- AUTO_INCREMENT для таблицы `lessons`
--
ALTER TABLE `lessons`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=5;

--
-- AUTO_INCREMENT для таблицы `migrations`
--
ALTER TABLE `migrations`
  MODIFY `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=10;

--
-- AUTO_INCREMENT для таблицы `personal_access_tokens`
--
ALTER TABLE `personal_access_tokens`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT;

--
-- AUTO_INCREMENT для таблицы `questions`
--
ALTER TABLE `questions`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=3;

--
-- AUTO_INCREMENT для таблицы `recipes`
--
ALTER TABLE `recipes`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=11;

--
-- AUTO_INCREMENT для таблицы `users`
--
ALTER TABLE `users`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=4;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
